%Front;
\documentclass{article}
\usepackage{amsmath}
\begin{document}
	
\section{Distributions}

% beta already included in lecture 1

Dirichlet distribution; $p(\mathbf{x}|\mathbf{\alpha}) = \frac{\Gamma (\sum_{d=1}^{D}\alpha_d)}{\prod_{d=1}^{D}\Gamma(\alpha_d)}\prod_{d=1}^{D}x_d^{\alpha_{d}-1}$, $\mathbf{x}\in [0,1]^D$, $\mathbf{x}\sim Dirichlet(\alpha)$

Gamma distribution; $p(x|\alpha, \beta) = \frac{\beta^{\alpha}}{\Gamma(\alpha)}x^{\alpha-1}e^{-\beta x}$, $x\in \Re_+$

Poisson distribution; $p(x|\mu) = \frac{\mu^xe^{-\mu}}{x!}$, $x\in\mathcal{Z}_{0+}$

Multinomial distribution; $p(\mathbf{x|p})=\frac{N!}{x_1!x_2!...x_D!}\prod_{d=1}^{D}p_d^{x_d}$, $\mathbf{x}\sim [\mathcal{Z}_{0-N}]^D$

Binomial distribution; $p(x|p) = {N\choose x} p^x(1-p)^{(N-x)}$

Multivariate Gaussian distribution; $p(\mathbf{x}|\mathbf{\mu, \Sigma}) = |2\pi\Sigma|^{-1/2}\exp(-\frac{1}{2}(\mathbf{x-\mu})^T\Sigma^{-1}(\mathbf{x-\mu})$

What is one way of making generative models that are more expressive than those in the exponential family (but still involves using exponential family dists)?; Combining and transforming latent exponential family variates.

What is the structure of a simple two-variable latent variable model?; \begin{itemize} \item Suppose $y~P[\theta_y]$, $x|y~P[\theta_x]$.  \item $p(x,y|\theta_x, \theta_y) = p(x|y, \theta_x)p(y|\theta_y)$ \item $p(x|\theta_x, \theta_y) = \int dy p(x|y, \theta_x) p(y|\theta_y)$.  \end{itemize}

Why might we use latent variable models?; \begin{itemize} \item Describe structured distributions \begin{itemize} \item correlations in high-dim x may be captured by fewer parameters.  \end{itemize} \item Capture an underlying generative process \begin{itemize} \item y may describe causes of x.  \item help to separate signal from noise.  \end{itemize} \item Combine exponential family dists into richer, more flexible forms.  \begin{itemize} \item P(y), P(x|y) and even P(x,y) may be in the exponential family \item P(x) rarely is. (Exception: linear Gaussian models) \end{itemize} \end{itemize}

How can we decompose a Gaussian with correlated noise, e.g. $x~N(0, [ 3 2 / 2 3])$?; $y~N(0,1)$, $x~N(\sqrt{2}[1 / 1]y, I)$.

Describe the probabilistic PCA model.; \begin{itemize} \item N D-dim datapoints $\mathbf{x_i} \in \mathcal{R}^D$ \item N k-dimensional latents $y_i \in \mathcal{R}^K$ \item Linear generative model: $x_d = \sum_{k=1}^k \Lambda_{dk}y_k + \epsilon_d$, where \begin{itemize} \item $y_k$ are independent $N(0,1)$ Gaussian factors \item $\epsilon_d$ are independent $N(0,\psi)$ Gaussian noise \item latent dim K $<$ D (obs dim) \item $\Lambda$ is a D x K matrix.  \end{itemize} \item $p(x|y) = N(\Lambda y,\phi I)$ \item $p(x) = \int p(y)p(x|y)dy = N(E_y[\Lambda y], E_y[\Lambda y y^T\Lambda^T]+\psi I)/N(0,\Lambda\Lambda^T+\psi I)$ \end{itemize}

Law of iterated expectation; $E_x[f(x)] = E_y[E_{x|y}[f(x)]]$

Variance of x in terms of expectation, variance over y, where x can be conditioned on y; $Var_X[x] = E_Y[V_X[x|y] + V_Y[E[x|y]]]$

Compare modelling x as a multivariate Gaussian vs using probabilistic PCA; \begin{itemize}\item Multivariate Gaussian \begin{itemize} \item Model: Descriptive density model: correlations captured by off-diagonal elements of $\Sigma$.  \item $\Sigma$ has $\frac{D(D+1)}{2}$ free parameters.  \item $\Sigma$ only constrained to be positive definite.  \item Simple ML estimate.  \end{itemize} \item Latent variables \begin{itemize} \item Interpretable causal model: correlations caputerd by common influence of latent variable.  \item $\Lambda\Lambda^T +\psi I$ has $DK+1$ free parameters.  \item For $K < D$, covariance structure (of x?) is constrained (blurry pancake) \item ML estimation is more complex.  \end{itemize} \end{itemize}

Probabilistic PCA log likelihood (over entire dataset); $\log p(\chi | \Lambda, \psi) = -\frac{N}{2}\log|2\pi(\Lambda\Lambda^T+\psi I)|-\frac{1}{2}Tr[(\Lambda\Lambda^T+\psi I)^{-1}\sum_n\mathbf{x x^T}]$

[TODO: how to do maximum likelihood over PPCA likelihood for $(\Lambda, \psi)$?] (can do gradient ascent, but there is a 'simpler principle')

Link between probabilistic PCA and PCA; PPCA $\to$ PCA when $\psi\to 0$, where $\psi$ is the magnitude of the additive noise to x conditional on y.

Describe PCA; Assume data have zero mean. \begin{itemize} \item Find direction of greatest variance $\mathbf{\lambda}_{(1)}=\arg\max\limits_{||\mathbf{v}||=1}\sum_n(\mathbf{x^T_nv})^2$ \item Find direction orthogonal to $\mathbf{\lambda}_{n}$ with greatest variance, call this  $\mathbf{\lambda}_{n+1}$.  \item Terminate when remaining variance drops below a threshold.  \item These turn out to be the eigenvectors of the empirical covariance matrix $S=\langle \mathbf{xx^T} \rangle$ \end{itemize}

Express in algebraic terms the fact that the D eigenvectors form an orthonormal basis.; $\sum_i\mathbf{u_{i}u_{i}^T}=I$.

Rewrite a vector $v$ in terms of the orthonormal basis vectors $u$.; \begin{itemize} \item $\mathbf{v}=(\sum_i \mathbf{u_{(i)}u_{(i)}^T})\mathbf{v}$ \item $=\sum_i (\mathbf{u_{(i)}^Tv)u_{(i)}}$ \item $=\sum_i v_{(i)}\mathbf{u_{(i)}}$ \end{itemize}

Rewrite the empirical covariance matrix $S=\langle \mathbf{xx^T} \rangle$ in terms of the orthogonal basis vectors $\mathbf{u}_i$.; $S=\sum_i \omega_{(i)}\mathbf{u}_{(i)}\mathbf{u}_{(i)}^T = UWU^T$, where \begin{itemize} \item $U=[\mathbf{u_{1}}...]$ collects the eigenvectors and \item $W = diag[(\omega_{(1)}, \omega_{(2)},...,\omega_{(D)})]$.  \end{itemize}

How many eigenvalue-eigenvector pairs does the covariance matrix $S_{DXD}$ have?; \begin{itemize} \item Usually D pairs, \item except if two or more eigenvectors share the same eigenvalue (in which case the eigenvectors are degenerate - any linear combination is also an eigenvector).  \end{itemize}

Variance of empirical data in direction $\mathbf{u}_{(i)}$ (PCA); \begin{itemize} \item $\langle (\mathbf{x^Tu_{(i)}})^2\rangle $ \item $= \langle \mathbf{u_{(i)}^Txx^Tu_{(i)}}\rangle $ \item $=\mathbf{u_{(i)}^TSu_{(i)}}$ \item $=\mathbf{u_{(i)}^T\omega_{(i)}u_{(i)}}$ \item $=\omega_{(i)}$ \end{itemize}

Variance of empirical data in an arbitrary direction $\mathbf{v}$; \begin{itemize} \item $\langle (\mathbf{x^Tv})^2\rangle $ \item $= \langle (\mathbf{x^T(\sum_i v_{(i)}\mathbf{u}_{(i)})})^2\rangle $ \item $=\sum_{ij}v_{(i)}\mathbf{u_{(i)}^TSu_{(j)}}v_{(j)}$ \item $=\sum_{ij}v_{(i)}\omega_{(i)}v_{(j)}\mathbf{u_{(i)}^Tu_{(j)}}v_{(j)}$ \item $=\sum_i v_{(i)}^2\omega_{(i)}$ \end{itemize}

Explain how the PCs can be obtained from the empirical data.; \begin{itemize} \item PCs are the eigenvectors of the empirical covariance matrix $S=\langle\mathbf{xx^T}\rangle$ with the largest eigenvalues.  \item Variance in arbitrary direction is $\langle (\mathbf{x^Tv})^2\rangle =\sum_i v_{(i)}^2\omega_{(i)}$.  \item If $\mathbf{v^Tv}=1$, then $\sum_i v_{(i)}^2=1$ and so $\arg\max_{||v||=1} \langle (\mathbf{x^Tv})^2\rangle = \mathbf{u}_{(max)}$. ($v_{(i)}=1$ corresponding to largest eigenvalue, zero otherwise.) \item i.e. the direction of greatest variance is the eigenvector with the largest eigenvalue.  \end{itemize}

State the algebraic expression for a projection into a K-dimensional principal subspace.; $\hat{x_n} = \sum_{k=1}^K x_{n(k)}\mathbf{\lambda}_{(k)}$

State three equivalent definitions of PCA.; \begin{itemize} \item Find K directions of greatest variance in data.  \item Find K-dimensional orthogonal projection that preserves greatest variance.  \item Find K-dimensional vectors $y_i$ and matrix $\Lambda$ so that $\hat{\mathbf{x}}_i = \Lambda\mathbf{y_i}$ is as close as possible (in terms of squared distance) to $\mathbf{x_i}$.  \end{itemize}

Describe PCA from the perspective of mutual information.; \begin{itemize} \item Problem: Given $\mathbf{x}$, find $\mathbf{y = Ax}$ with columns of $A$ being unit vectors such that $I(y,x)$ is maximised, assuming $p(\mathbf{x})$ is Gaussian. (Also dim y $<$ dim x.) \item I(y,x) = H(y) + H(x) - H(x,y). Here, H(x) = H(x,y) so I(x,y) = H(y).  \item $H(y)=-\int dz p(z) \ln p(z) = \frac{1}{2}\ln|\Sigma|+\frac{D}{2}(1+\ln 2\pi )$.  \item I.e. want to choose $y$ with largest det of covariance matrix $|\Sigma|$, i.e. largest volume.  \item $\Sigma_y = A\Sigma_x A^T = AUW_XU^TA^T$ \item So $A$ should be aligned with the columns of $U$ which are associated with the largest eigenvalues (variances).  \item Projection to the principal component subspace preserves the most information about the data.  \end{itemize}

How do you interpret a matrix? e.g. $U = [u_1 u_2 ...]$, and you have the product $Ux$.; Each $u_i$ is an axis (or directional vector), and each $x_i$ is how far you want to walk in the direction of $u_i$. \newline Note: this is not in the notes.

Briefly describe linear autoencoders.; \begin{itemize} \item Learn $\arg\min_{P,Q}||\hat{\mathbf{x}}-\mathbf{x}||^2, \mathbf{\hat{x}=Qy}$ (decoder 'generation'), $\mathbf{y=Px}$ (encoder 'recognition').  \item At the optimum, P and Q perform the projection and reconstruction steps of PCA (Baldi and Hornik 1989).  \end{itemize}

Derivative of a matrix inverse $\frac{d\mathbf{Y^{-1}}}{dx}$; $-Y^{-1}\frac{\partial{Y}}{dx}Y^{-1}$

Rank-Nullity theorem; rank(S) + Nullity(S) = dim(V), where $S:V\to W$. i.e. V is number of columns of matrix. k

Derivative of trace of matrix inverse multiplied by other matrices $\frac{d Tr \mathbf{(AX^{-1}B)^{-1}}}{dX}$; $-(X^{-1}BAX^{-1})^T$

Properties of an empirical covariance matrix; \begin{itemize}
	\item DxD, where D is the dimension of the data $x$
	\item Symmetric
	\item Positive semi-definite
\end{itemize}

Log likelihood of PPCA; $l = -\frac{N}{2}\log|2\pi C|-\frac{N}{2}Tr[C^{-1}S]$, where $C=\Lambda\Lambda^T+\psi I$.

Maximum likelihood expression for PPCA $\Lambda$: \begin{itemize}
	\item $\frac{\partial l}{\partial \Lambda} = N(-C^{-1}\Lambda+C^{-1}SC^{-1}\Lambda) = 0$
	\item (Use trace inverse trick.)
	\item So at stationary point we have $\Lambda = SC^{-1}\Lambda$.
	\item working: $l = -\frac{N}{2}\log|2\pi C|-\frac{N}{2}Tr[C^{-1}S]$, where $C=\Lambda\Lambda^T+\psi I$
	\item and use trace inverse formula $\frac{d Tr \mathbf{(AX^{-1}B)^{-1}}}{dX}] = -(X^{-1}BAX^{-1})^T$
\end{itemize}

Singular Value Decomposition; SVD decomposes any n x p matrix $\mathbf{X}$.\begin{itemize}
	\item $X = USV^T$
	\item dim U = nxn, $U^TU=I_n$.
	\item dim V = pxp, $V^TV=I_p$
	\item dim S = nxp, S diagonal. 
	\item Singular values are diagonal entries $[S]_{jj}$, all positive, ordered in decreasing order from upper left diagonal element.
\end{itemize} (From Intro to DL Background Maths slides)

ML learning of $\Lambda$ for PPCA: what to do after obtaining stationary point expression $\Lambda = SC^{-1}\Lambda, C=(\Lambda\Lambda^T+\psi I)$; \begin{itemize}
	\item Either (1) $\Lambda=0$, which turns out to be a minimum, or
	\item (2) $C=S \Rightarrow \Lambda\Lambda^T=S-\psi I$
	\begin{itemize}
		\item Method 1: $rank(\Lambda\Lambda^T)\leq K\Rightarrow rank(S-\psi I) \leq K$
		\item $\Rightarrow S$ has $D-K$ eigenvalues $=\psi$ and $\Lambda$ aligns with the space of remaining eigenvectors.
		\item Method 2: Take the SVD $\Lambda = ULV^T$ on stat pt expression, use trick $U(L^2+\psi I)=(UL^2UU^T+\psi I)U$ and take inverses of both sides to get $SU=U(L^2+\psi I), (L^2+\psi I)$ diagonal.
		\item So columns of U are eigenvectors of S with eigenvalues given by $l^2_i + \psi$. 
		\item Thus, $\Lambda = ULV^T$ spans a space defined by K eigenvectors of S, 
		\item and the lengths of the column vectors of L are given by the eigenvalues $-\psi$ (V selects an arbitrary basis in the latent space).
	\end{itemize}
\item Remains to show that the global ML solution is attained when $\Lambda$ aligns with the K leading eigenvectors. (claims this is intuitively reasonable.)
\end{itemize}

Can we project $x_n \rightarrow \hat{x_n}$ trivially in PCA and in PPCA?; \begin{itemize}
	\item PCA yes since 'noise' is orthogonal to subspace.
	\item PPCA no since noise is equal in all directions.
\end{itemize}

Finding the projection for PPCA; \begin{itemize}
	\item Find the expected value $\mathbf{\bar{y}}_n=E[\mathbf{y_n|x_n}]$ and then take $\hat{x}_n=\Lambda\bar{y}_n$.
	\item Find expected value by taking $p(y_n, x_n|\theta)$ and considering $x_n$ to be fixed.
	\item Then get $p(y_n, x_n) \sim N(\mu, \Sigma)$, where $Sigma = (I+\Lambda^T\Psi^{-1}\Lambda)^{-1}=I-\beta\Lambda$ and $\mu=\Sigma\Lambda^T\Psi^{-1}x_n=\beta x_n$, where $\beta = \Sigma\Lambda^T\Psi^{-1}$.
	\item Above: use $(I+P)^{-1}=I-(I+P)^{-1}P$.
	\item Thus, $\hat{x}_n=\Lambda\mu=x_n - \Psi(\Lambda\Lambda^T+\Psi)^{-1}x_n$.
	\item Above: use $(I+PQ)^{-1}P=P(I+QP)^{-1}$, and then add I and take away I, $I=\Psi\Psi^{-1}$.
	\item Not the same projection as PCA. PPCA takes into account noise in the principal subspace.
	\item As $\psi\rightarrow 0$, PPCA est approaches PCA value.
\end{itemize}

\end{document}
