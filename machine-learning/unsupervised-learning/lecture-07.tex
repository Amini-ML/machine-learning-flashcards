%Front;
% Approximate Inference
% Lectured by Maneesh Sahani at the Gatsby Computational Neuroscience Unit, UCL, Fall 2018
% Link to course info and slides: http://www.gatsby.ucl.ac.uk/teaching/courses/ml1/

\documentclass{article}
\begin{document}

% TODO: a generative model of generative models diagram

Why do we need non-linear or non-Gaussian models? Give an example.; \begin{itemize}
    \item Much of the world is neither linear nor Gaussian.
    \item E.g. the joint distribution of pixel brightnesses in greyscale images of natural scenes is sparsely distributed.
\end{itemize}

Give one reason why hierarchical models may be useful.; \begin{itemize}
    \item Many generative processes (e.g. biological ones) can be naturally described at different levels of detail. 
    \item e.g. images: High to low level: objects, illumination, pose $\rightarrow$ object parts, surfaces $\rightarrow$ edges $\rightarrow$ retinal image, e.g. pixels
    \item Biology seems to have developed hierarchical representations.
\end{itemize}

Briefly describe distributed representations; \begin{itemize}
    \item Each observation is described by a vector of (discrete or continuous) attributes.
    \item Some of these attributes may be latent.
\end{itemize}

Briefly discuss the efficiency of distributed representations; \begin{itemize}
    \item Can be exponentially efficient: K binary factors $\Rightarrow 2^K$ bits of info.
\end{itemize}

Does it make sense for a linear Gaussian model to have hierarchicies with discrete or non-gaussian dependences?; No because this removes the benefits of having a linear Gaussian model in the first place (easy to calculate). (TODO check)

Briefly describe blind source separation.; \begin{itemize}
    \item Given signals from one or more receivers 
    \item that mix signals from one or more sources,
    \item recover the time series of the source signals.
    \item Sometimes called the cocktail party problem.
\item (Don't know where source is in room so don't know what transforms to apply)
\end{itemize}

How are pixel brightnesses of greyscale natural images distributed?; Sparse, like Laplace dist.

Describe independent components analysis (ICA); \begin{itemize}
    \item Graphical model identical to factor analysis: $\x_d=\sum_{k=1}^K\Lambda_{dk}y_k+\epsilon_d$
    \begin{itemize}
        \item I.e. linear combination of factors
        \item Independent $\epsilon_d\sim N(0, \Psi_{dd})$ Gaussian noise
    \end{itemize}
    \item but with sources/factors $y_k\sim P_i$ non-Gaussian.
\end{itemize}

Name four differences between ICA and Factor Analysis; (things below are ICA) \begin{itemize}
    \item Sources/factors in ICA are non-Gaussian (vs FA Gaussian)
    \item ICA well-posed even with $K\geq D$| since there is no rotational symmetry
    \item With non-zero noise, MAP inference is non-linear, and the full posterior is non-Gaussian (vs FA posterior Gaussian)
    \item $\Rightarrow$ exact inference and learning in ICA difficult for most $P_y$.
\end{itemize}

Describe what graphs of ICA-generated data looks like with two sources $y$.; \begin{itemize}
    \item Heavy-tailed sources: like two crossing lines with much data clustered in the intersection. Each line is a $\Lambda$ dim.
    \item Light-tailed sources: like a parallelogram. Each `side' is a $\Lambda$ dim.
    \item (vs Gaussian would be elliptical.
\end{itemize}

Describe square, noiseless ICA; \begin{itemize}
    \item K=D, i.e. number of sources = number of observation dimensions.
    \item AND zero observation noise.
    \item equivalent to infomax ICA
    \item have $\mathbf{x=\Lambda y}$, which implies $\mathbf{y=Wx}$ with $W=\Lambda^{-1}$. (if $\Lambda$ is full rank)
    \begin{itemize}
        \item W is the unmixing matrix
    \end{itemize}
    \item ($P(\mathbf{x}|W)=|W|\prod_kP_i([W\mathbf{x}]_k)$.) TODO: okay, so what? independent-seeming?
\end{itemize}

Derive the likelihood $P(\mathbf{x}|W)$ for square, noiseless ICA, where $W=\Lambda^{-1}$ is the unmixing matrix.; \begin{itemize}
    \item Obtain likelihood by transforming the density of $\mathbf{y}$ to that of $\mathbf{x}$.
    \begin{itemize}
        \item If $F:\mathbf{y}\to\mathbf{x}$ is a differentiable bijection, and if $d\mathbf{y}$ is a small neighbourhood around $\mathbf{y}$, then
        \item $P_X(\mathbf{x})d\mathbf{x}=P_y(\mathbf{y})d\mathbf{y}=P_y(F^{-1}(\mathbf{x}))|\frac{d\mathbf{y}}{d\mathbf{x}}|d\mathbf{x}=P_y(F^{-1}(\mathbf{x}))|\nabla F^{-1}|d\mathbf{x}$
        \begin{itemize}
            \item Transfer x to y
            \item Size of volume 
            \item note $P_y$ (marginal dist of factors )is given in the model
            \item (Something illegible about not intract? y bc model datasets? so a delta fn)
        \end{itemize}
    \end{itemize}
    \item $P(\mathbf{x}|W)=|W|\prod_kP_i([W\mathbf{x}]_k)$.
\end{itemize}

What is the log likelihood of data for ICA?; $\log P(\mathbf{x})=\log|W|+\sum_i \log P_y(W_i\mathbf{x})$

How could you learn ICA? \begin{itemize}
    \item Log likelihood: $\log P(\mathbf{x})=\log|W|+\sum_i \log P_y(W_i\mathbf{x})$ 
    \item Learn by gradient ascent: $\Delta W\propto \nabla_W\log P(\mathbf{x}) = W^{-T}+g(\mathbf{y})\mathbf{x}^T$
    \begin{itemize}
        \item W map from x to y
        \item units: y / x
        \item $\mathbf{x}^T$ is $\frac{\partial W_ix}{\partial W}$
        \item $g(y)=\frac{\partial \log P_y(y)}{\partial y}$
    \end{itemize}
    \item or learn by `natural' or `covariant' gradient $\Delta W\propto \nabla_W\log P(\mathbf{x})\cdot (W^TW) = W + g(\mathbf{y})\mathbf{y}^TW \approx \langle -\nabla\nabla\log P\rangle^{-1}$
    \begin{itemize}
        \item Naturally covariant to scaling and rotations
        \item $\nabla\nabla$: Hessian
        \item $W^TW$: average value of 2nd derivative, if exact then is Newton method
        \item natural grad no units vs learning by gradient ascent have units y/x.
        \item $g(y)=\frac{\partial \log P_y(y)}{\partial y}$
    \end{itemize}
\end{itemize}

Write down the natural gradient for ICA; \begin{itemize}
    \item or learn by `natural' or `covariant' gradient $\Delta W\propto \nabla_W\log P(\mathbf{x})\cdot (W^TW) = W + g(\mathbf{y})\mathbf{y}^TW \approx \langle -\nabla\nabla\log P\rangle^{-1}$
    \begin{itemize}
        \item Naturally covariant to scaling and rotations
        \item $\nabla\nabla$: Hessian
        \item $W^TW$: average value of 2nd derivative, if exact then is Newton method
        \item natural grad no units vs learning by gradient ascent have units y/x.
        \item $g(y)=\frac{\partial \log P_y(y)}{\partial y}$
    \end{itemize}
\end{itemize}

Why is the natural gradient called natural gradient?; bc it's naturally covariant to scaling and rotations.

% more illegible notes on learning in ICA slide (s19)

Can we use EM in the square noiseless causal ICA model? Why?; 

Describe Infomax ICA; \begin{itemize}
    \item Consider a feedforward model $y_i=W_i\mathbf{x}$, $z_i=f_i(y_i)$
    with a monotonic squashing function $f_i(\infty)=0, f_i(+\infty)+1$. (like sigmoid)
    \item Infomax finds filtering weights W maximising the info carried by $\mathbf{z}$ about $\mathbf{x}$: \begin{itemize}
        \item $\arg\max_W I(\mathbf{x, z}) = \arg\max_W H(\mathbf{z})-H(\mathbf{z|x})=\arg\max_WH(\mathbf{z})$.
        \item $H(\mathbf{z})=0$ bc it's a deterministic mapping
        \item so we just have to maximise entropy on z (make it as uniform as possible on [0, 1]
    \end{itemize}
    \item but if data were generated from a square noiseless causal ICA then best we can do is if $z_i=f_i(y_i)=cdf_i(y_i)$ and $W=\Lambda^{-1}$
    \begin{itemize}
        \item since cdf is uniform.
        \item by defn, $x=\Lambda y$
    \end{itemize}
    \item so infomax ica is equivalent to square noiseless causal ICA
    \item (Another view: redundancy reduction in the representation $\mathbf{z}$ of the data $\mathbf{x}$.
    \begin{itemize}
        \item $\arg\max_W H(\mathbf{z})=\arg\max_W\sum_i H(z_i)-I(z_1,...,z_D)$.
    \end{itemize}
\end{itemize}

Describe kurtosis; \begin{itemize}
    \item Kurtosis measures how `peaky' or `heavy-tailed' a distribution is.
    \item (FYI: $K=\frac{E((x-\mu)^5)}{E((x-\mu)^2)^2}-3$, where $\mu=E(x)$ is the mean of $x$.)
\end{itemize}

Describe positive vs negative kurtosis distributions.; \begin{itemize}
    \item Positive kurtosis: heavy tailed (leptokurtic)
    \item Negative kurtosis: light tailed (platykurtic, flat)
\end{itemize}

What is the kurtosis for Gaussian distributions?; Zero

Name one kind of algorithm that takes a kurtosis pursuit approach.; ICA algorithms

TODO: something about possibly ICA not being a limiting case of CLT: linear mixtures of independent non-Gaussian dists: more G, is K tending towards zero?

Name at least two ways you can extend ICA.; \begin{itemize}
    \item Non-zero output noise: intractable, so have to approximate posteriors and learning
    \item Undercomplete $(K<D)$ or overcomplete $(K>D)$. (So not simple inverse rel but effectively like pseudoinverse)
    \item Learning prior dists (on $\mathbf{y}$) \begin{itemize}
        \item assumed know $p(\mathbf{y})$, but doesn't matter bc of kurtosis assumption. e.g. ML on high kurtosis usually ok (?)
    \end{itemize}
    \item Learning number of sources \begin{itemize}
        \item Possible if full prob model with noise, else post hoc look at something and decide if it's signal or noise
    \end{itemize}
    \item Time-varying mixing matrix
    \item Non-parametric, kernel ICA
\end{itemize}

Name one solution to blind source separation; ICA. Assumes no dependence across time, still works fine much of the time.

Describe a nonlinear state-space model (NLSSM); \begin{itemize}
    \item $\mathbf{y}_{t+1}=f(\mathbf{y}_t, \mathbf{u}_t)+\mathbf{w}_t$
    \item $\mathbf{x}_t=g(\mathbf{y}_t, \mathbf{u}_t)+\mathbf{v}_t$
    \item $\mathbf{w_t, v_t}$ usually still Gaussian.
\end{itemize}

Describe the Extended Kalman Filter (EKF) (with equations); \begin{itemize}
    \item Linearise (taylor expand) nonlinear functions about current estimate, $\mathbf{\hat{y}}^t_t$:
    \item $\mathbf{y}_{t+1}\approx f(\mathbf{\hat{y}}^t_t, \mathbf{u}_t) + \frac{\partial f}{\partial \mathbf{y}_t}|_{\hat{\mathbf{y}}^t_t}(\mathbf{y_t-\hat{y}^t_t})+\mathbf{w}_t$
    \item $\mathbf{x}_{t}\approx g(\mathbf{\hat{y}}^{t-1}_t, \mathbf{u}_t) + \frac{\partial g}{\partial \mathbf{y}_t}|_{\hat{\mathbf{y}}^{t-1}_t}(\mathbf{y_t-\hat{y}^{t-1}_t})+\mathbf{v}_t$
    \item Run the Kalman filter/smoother on non-stationary linearised system
    \item Adaptively approximates non-Gaussian messages by Gaussians
    \item Local linearisation depends on central point of distribution$\Rightarrow$ approx degrades with increased state uncertainty. May work acceptable for close-to-linear systems.
\end{itemize}

Describe the Extended Kalman Filter (without equations); \begin{itemize}
     \item Linearise (taylor expand) nonlinear functions about current estimate, $\mathbf{\hat{y}}^t_t$:
    \item Run the Kalman filter/smoother on non-stationary linearised system
    \item Adaptively approximates non-Gaussian messages by Gaussians
    \item Local linearisation depends on central point of distribution$\Rightarrow$ approx degrades with increased state uncertainty. May work acceptable for close-to-linear systems.
\end{itemize}

Describe how to do online parameter learning in NLSSMs; \begin{itemize}
    \item Nonlinear message passing
    \item E.g. for linear model (generalise to nonlinear by linearising), augment state vector to include model parameters: $\bar{\bar{\mathbf{y}}}=[\mathbf{y}_t A C]$
    \item $\bar{\bar{y}}_{t+1}=[A\mathbf{y}_t A C]^T + [\mathbf{w}_t 0 0 ]^T$
    \item $\mathbf{x}_t=C\mathbf{y}_t+\mathbf{v}_t$
    \item Use EKF to compute online estimates of $E[\bar{\bar{\mathbf{y}}}_t|\mathbf{x_1,...,x_t}$ and $Cov[\bar{\bar{\mathbf{y}}}_t|\mathbf{x_1,...,x_t}$. (TODO: how?) These now include mean and posterior variance of parameter estimates.
    \item Pseudo-Bayesian since gives Gaussian dists over params
\end{itemize}

Discuss the online EKF/joint-EKF approach.; \begin{itemize}
    \item Can model nonstationarity by assuming non-zero innovations noise in A, C
    \item Not simple to implement for Q and R (e.g. covariance constraints?) (todo: how do you even implement this for Q and R?)
    \item May be faster than EM/gradient approaches
\end{itemize}

\end{document}