%Front;
% Approximate Inference
% Lectured by Maneesh Sahani at the Gatsby Computational Neuroscience Unit, UCL, Fall 2018
% Link to course info and slides: http://www.gatsby.ucl.ac.uk/teaching/courses/ml1/
\documentclass{article}
\usepackage{amssymb, bm}
\begin{document}

% Recap: distributed representations of posterior. (For these: often don't have an easy way to spend more compute to get closer to the true distribution, unlike sampling) Approximate distributions on parameters. Approx may involve sampling. 

Describe two ways in which integrals may be intractable; \begin{itemize} \item Analytic intractability: integrals may not have closed form in non-linear, non-Gaussian models. Soln: numerical integration.  \item Computational intractability: numerical integral may be exponential in data or model size. (Focus of this lecture) \end{itemize}

Is the marginal likelihood for MoG tractable? Explain.; \begin{itemize} \item No. (Computationally intractable.) \item Exact computations are exponential in the number of data points: \item $p(\bm{x_1, ... x_N}) = \int d\theta p(\theta)\prod_{i=1}^N\sum_{s_i}p(\bm{x_i}|s_i,\theta)p(s_i|\theta)$ \item $= \sum_{s_1}...\sum_{s_N}\int d\theta p(\theta)\prod_{i=1}^Np(\bm{x_i}|s_i,\theta)p(s_i|\theta)$ \item need to compute $\mathtt{num-s-settings}^N$ integrals, each with prod of N terms.  \end{itemize}

Is computing the conditional probabilities in a large multiply-connected DAG tractable? Explain.; \begin{itemize} \item No. (Computationally intractable.) \item $p(x_i|X_j=a)=\sum_{\mathtt{all-settings-y-except-i-j}}p(x_i, \bm{y}, X_j=a)/p(X_j=a)$ \item Need to sum over all settings of y, which is exponential in dimension of y. (TODO: what is dim of y? num other nodes or sth?) \end{itemize}

Is computing the hidden state distribution in a general nonlinear dynamical system tractable?; \begin{itemize} \item No. (Computationally intractable.) \item $p(\bm{y_t|x_1, ..., x_t})\propto \int d\bm{y}_{t-1}p(\bm{y_t}|f(\bm{y}_{t-1}))p(\bm{x}_t|g(\bm{y}_t))p(\bm{y}_{t-1}|\bm{x_1,...,x_{t-1}})$ \item Q:is this intractable because summing over the $y_{t-1}$ is intractable? bc the latter part of the expression is recursive. And assuming we have $p(x_t|g(y_t))$ from somewhere else.  \end{itemize}

TODO: L8 VB slide 4 distributed models (FHMM sum); TODO

What does the E in EM stand for?; \begin{itemize} \item Expectation \item Because we only need to calculate the expectations of the sufficient statistics to do the updates if our distributions are in the exponential family.  \end{itemize}

Where might intractability in EM come from (in general)?; \begin{itemize} \item q is intractable \item Finding expected values of sufficient statistics (under $p(Y|X,\theta)$ for M-step) is intractable \end{itemize}

Where might intractability in EM come from for the M-step on a graphical model?; \begin{itemize} \item Difficulty of computing marginal posteriors \item in graphs with large tree-width or non-linear/non-conjugate conditionals.  \item TODO:add an example.  \item Non-DAG:partition function (normalising constant) may also be intractable.  \begin{itemize} \item recall: need expected sufficient stats from marginal posteriors on each factor group.  \item Non-DAG only since for DAGs, can optimise each factor parameter vector separately. (So?) \end{itemize} \end{itemize}

What are the key ideas behind variational approximation E-steps?; \begin{itemize} \item Parameterise $q=q_{\rho}(Y)$ and take a gradient step in $\rho$.  \item Assume some simplified form for $q$, usually factored: $q=\pi_i q_i(Y_i)$ where $Y_i$ partition $Y$, and maximise within this form.  \begin{itemize} \item focus of this lecture \item Can write as $q^{(k)}(Y):=\arg\max_{q(Y)\in\mathcal{Q}}\mathcal{F}(q(Y),\theta^{(k-1)})$, where $\mathcal{Q}$ is the constraint set.  \item Changes optimisation problem (vs gradient step does not).  \item So the fixed point this arrives at may not be at an unconstrained optimum of $\mathcal{F}$.  \end{itemize} \end{itemize}

Describe the differences (in results) between variational approx E-step and general EM.; \begin{itemize} \item Since $p(\mathcal{Y}|\mathcal{X},\theta^{(k)})$ may not lie in $\mathcal{Q}$, we no longer have $\mathcal{F}=l$ after the E-step.  \item Thus the likelihood may not increase on each full EM step (since we only know F increases, and during E-step likelihood may decrease.) \item So we may not converge to a maximum of l.  \item Idea is that by increasing a lower bound on l, we may find a decent solution.  \end{itemize}

Describe the nature of the result obtained from the variational approx E-step in terms of the expression for F.; \begin{itemize} \item EM soln: A compromise between maximising the likelihood $P(X|\theta)$ and minimising the $KL(q||p)$ since the KL is no longer zero after the E-step.  \item E-step: min $KL(q||p(Y|X,\theta^{(k-1)}))$.  \item Min KL component can be seen as a bias in $\theta$ drawing the approx form closer to the posterior.  \item (And since approx anyway, suggests generalisation to other divergence measures.) \item (In practice could try early stopping based on test/val set preds? assuming this stopping refers to E-step iterations?) \end{itemize}

Describe the factored variational E-step; \begin{itemize} \item Partition latents $\mathcal{Y}$ into disjoint sets $Y_i$ with \item $\mathcal{Q}=\{q|q(Y)=\prod_i q_i(Y_i)\}$ \item Max $\mathcal{F}(q,\theta)$ wrt $q_i(Y_i)$ given other $q_j$ and parameters: \item $q_i^{(k)}(Y_i)=\arg\max_{q_i(Y_i)}\mathcal{F}(q_i(Y_i)\prod_{j\ne i}q_j(Y_j),\theta^{(k-1)})$.  \begin{itemize} \item $q_i$ updates iterated to convergence to 'complete' VE-step.  \item Every $VE_i$-step separately increases F, so any schedule of $VE_i$- and M-steps will converge.  \item So if M-step is very cheap, may do an M-step after each $VE_i$-step. Usually a few passes through E first.  \end{itemize} \end{itemize}

Derive the general form of $q_i(Y_i)$ for the factored var E-step.; \begin{itemize} \item FRee energy is $\mathcal{F}(\prod_j q_j(Y_j),\theta^{(k-1)}) = \langle \log P(X,Y|\theta^{(k-1)})\rangle_{\prod_j q_j(Y_j)} +H[\prod_jq_j(Y_j)]$ \begin{itemize} \item = $\int dY_i q_i(Y_i)  \langle \log P(X,Y|\theta^{(k-1)})\rangle_{\prod_{j\ne i} q_j(Y_j)} +H[q_i] +\sum_{j\ne i}H[q_j]$ \end{itemize} \item Take variational derivative (diff wrt fn) of Lagrangian (enforcing normalisation of q i.e. int to 1 since it's a dist) \begin{itemize} \item $\frac{\delta}{\delta q_i} (\mathcal{F}+\lambda (\int q_i - 1)) =  \langle \log P(X,Y|\theta^{(k-1)})\rangle_{\prod_{j\ne i} q_j(Y_j)}- \log q_i(Y_i) - \frac{q_i(Y_i)}{q_i(Y_i)}+\lambda = 0$ \item $\Rightarrow q_i(Y_i) \propto \exp  \langle \log P(X,Y|\theta^{(k-1)})\rangle_{\prod_{j\ne i} q_j(Y_j)}$ \end{itemize} \item Generally depends only on the expected sufficient statistics under $q_j$ (vs need entire dist).  \end{itemize}

Intuition for variational derivatives (differentiating wrt a function); \begin{itemize} \item Discretise space of $Y_i$, so $q_i$ become vectors on space, with each dimension being a setting of $Y_i$.  \end{itemize}

Describe mean-field approximations for factored VE.; \begin{itemize} \item Factored approx: next q is what should those disjoint sets be? One answer is having each variable in its own set.  \item Strong assumption of `independence' so gap (KL) may be larger.  \item Intuit: Each var sees the 'mean field' (mean suff stats in general) of its neighbours/other vars (only affected by mean suff stats usually), \item and we update these fields (qs) until they all agree.  \end{itemize} 

What do mean-field approximations correspond to in physics?; Boltzmann machines or Ising model.

Write down the joint $P(X,Y)$ for the Boltzmann machine.; \begin{itemize} \item $P(X,Y)=\frac{1}{Z}\exp(\sum_{ij}W_{ij}s_is_j+\sum_ib_is_i)$, \item with some $s_i\in Y$ and others observed (in $X$).  \end{itemize}

What is the expectation over a fully factored q for a Boltzmann machine ($P(X,Y)=\frac{1}{Z}\exp(\sum_{ij}W_{ij}s_is_j+\sum_ib_is_i)$, some $s_i$ hidden, some observed)?; \begin{itemize} \item $\langle \log P(X,Y)\rangle_{\prod q_i} =\exp(\sum_{ij}W_{ij}\langle s_i\rangle_{q_i}\langle s_j\rangle_{q_j}+\sum_ib_i\langle s_i\rangle q_i$ \item where $q_i$ for $s_i \in X$ is a delta function on the observed value.  \item TODO: when did this become an equality?  \item So can update each $q_i$ in turn given the means (/in gen mean suff stats of others). Since all that affects one var from other var are the means.  \end{itemize}

Derive the q-distributions for the fully factored mean-field factorial HMM.; \begin{align*} q^m_t(s^m_t) &\propto \exp \langle \log P(\bm{s}^{1:M}_{1:T}, \bm{x}_{1:T})\rangle_{\prod_{\neg (m, t)}q^{m'}_{t'}(s^{m'}_{t'})} \\ &= \exp \langle \sum_{\mu}\sum_\tau\log P(s^\mu_\tau|s^\mu_{\tau-1}) + \sum_\tau\log P(\bm{x}_\tau|s^{1:M}_\tau)\rangle_{\prod_{\neg (m, t)}q^{m'}_{t'}} \\ &\propto \exp [\langle \log p(s^m_t|s^m_{t-1})\rangle_{q^m_{t-1}} + \langle \log p(\bm{x}_t|s^{1:M}_t)\rangle_{\prod_{\neg(m)}q^{m'}_{t'}} + \langle \log P(s^m_{t+1}|s^m_t)\rangle_{q^m_{t+1}} ]\\ &= \alpha^m_t(i)\beta^m_t(i).  \end{align*} (alpha first two terms (forward message), beta last term (backward message))

Describe the q-distributions for fully mean-field FHMM.; \begin{itemize} \item Yields a message-passing algorithm like forward-backward \item Updates depend only on immediate neighbours in chain \item Chains couple only through joint output $p(\bm{x}_t|s^{1:M}_t)$.  \item Need to have multiple passes to converge since changing one q changes all other qs. (Messages depend on approximate marginals (qs?)) \item Evidence $\log p(\bm{x}_t|s_t^{1:M}$ does not appear explicitly in backward message. (Think does not appear in Kalman smoothing bkwd message either.) \end{itemize}

Discuss explaining away in the fully mean-field FHMM; TODO check (think through \begin{itemize} \item Previously (assuming this means either Kalman smoothing or non-mean-field FHMM?) explaining away on explicit coupling (? what is coupling) \item Now explaining away part of observations s.t. only what was not explained away by others (is...?  \end{itemize}

Does $q(\mathcal{Y})$ need to be completely factorised for a variational approx? If not, what is needed or sufficient?; No. \begin{itemize} \item if s.t. computing expected sufficient statistics under $P(\mathcal{Y_i}|\mathcal{Y_{\neg i}, X})$ would be tractable, factored approx is tractable.  \item And any factorisation of $q(\mathcal{Y}$ into a product of distributions on trees yields a tractable approx. (so don't need junction tree. And no explaining away then.) \end{itemize}

Derive the q distributions for a structured FHMM (factor the chains); $q(s^{1:M}_{1:T}) = \prod_mq^m(s^m_{1:T})$, so \begin{align*} q^m(s^m_{1:T})&\propto \exp \langle \log P(\bm{s}^{1:M}_{1:T}, \bm{x}_{1:T})\rangle_{\prod_{\neg (m)}q^{m'}(s^{m'}_{1:T})} \\ &= \exp \langle \sum_{\mu}\sum_t\log P(s^\mu_t|s^\mu_{t-1}) + \sum_t\log P(\bm{x}_t|s^{1:M}_t)\rangle_{\prod_{\neg (m)}q^{m'}(s^{m'}_{1:T})} \\ &\propto \exp [ \sum_t\log P(s^m_t|s^m_{t+1}) + \sum_t\langle\log P(\bm{x}_t|s^{1:M}_t)\rangle_{\prod_{\neg (m)}q^{m'}(s^{m'}_{t})}] \\ &= \prod_t P(s^m_t|s^m_{t-1})\prod_t \exp{[\langle \log P(\bm{x}_t|s^{1:M}_t)\rangle_{\prod_{\neg (m)}q^{m'}(s^{m'}_{t})}] \\ \end{align*} \begin{itemize} \item This looks like a standard HMM joint with a modified likelihood term.  \item Iterate / cycle through multiple forward-backward passes, updating likelihood terms each time.  \item Opt commentary: Fuller dist of all states in chain. Take E(X)s wrt current setting of dist in all other chains. Second term in final expr is p(x) of single chain in context of all other chains. Looks like transition process for single chain.  \end{itemize}

Write down the joint $P(\mathcal{X, Y})$ for a DAG; $P(\mathcal{X, Y}) = \prod_k P(Z_k|\text{pa}(Z_k))$, where $k$ are the nodes.

Derive the general form of messages passed between nodes in the graph (derivation from a variational perspective).; Notation: $\{\mathcal{Y}_i\}$ are disjoint sets.  Let $q(\mathcal{Y}_i) = \prod_iq_i(\mathcal{Y}_i)$ for disjoint sets $\{\mathcal{Y}_i\}$. Then \begin{align*} q_i^*(\mathcal{Y}_i) &\propto \exp \langle \log P(\mathcal{Y, X}) \rangle_{q_{\neg i}(\mathcal{Y})} \\ \Rightarrow \log q^*_i(\mathcal{Y}) &= \langle \sum_k \log P(Z_k |\text{pa}(Z_k)) \rangle_{q_{\neg i}(\mathcal{Y})} + C\\ &= \sum_{j\in \mathcal{Y}_i} \langle\log P(Y_j |\text{pa}(Y_j)) \rangle_{q_{\neg i}(\mathcal{Y})} +  \sum_{j\in \text{ch}(\mathcal{Y}_i)} \langle \log P(Z_j |\text{pa}(Z_j)) \rangle_{q_{\neg i}(\mathcal{Y})} + C \end{align*} Each node receives messages from its Markov boundary: parents, children and parents of children (all neighbours in the corresponding factor graph).

What information do we need to calculate $q_i^*(\mathcal{Y}_i)$ for some disjoint set $\{\mathcal{Y}_i\}$ on a DAG?; Messages from the nodes' Markov boundary (parents, children, parents of children, i.e. neighbours in corresponding factor graph).

When is the term variational approximation used?; \begin{itemize} \item Whenever a bound on the likelihood (on another est cost function) is optimised \item but does not necessarily become tight.  \item (e.g. vs var opt in E-step F = likelihood, bound becomes tight.) \end{itemize}

What are majorising and minorising bounds?; \begin{itemize} \item Majorising: upper bound \item Minorising: lower bound \item (Just FYI) \end{itemize}

What is variational Bayes?; \begin{itemize} \item Q over both latents and parameters: $Q(\mathcal{Y}, \bm{theta})$.  \item Constraint: dist Q must factor into $Q_y(\mathcal{Y})Q_\theta(\bm{theta})$.  \end{itemize}

Compare Variational Bayes with plain EM.; \begin{itemize} \item Q over parameters as well as latents.  \item Approx (vs opt): Constraint: dist Q must factor into $Q_y(\mathcal{Y})Q_\theta(\bm{theta})$.  \item Goal: max bound on $P(\mathcal{X}|m)$ wrt $Q_\theta$ vs max $P(\bm{\theta}|\mathcal{X},m)$ wrt $\bm{\theta}$.  \item E-step: compute $Q_{\mathcal{Y}}(\mathcal{Y})\leftarrow p(\mathcal{Y}|\mathcal{X},\bm{\bar{phi}})$ vs $\bm{\theta}$ instead of $\bar{\bm{\phi}}$.  \item M-step: $Q_\theta(\bm{\theta}) \leftarrow \exp \int d\mathcal{Y}Q_\mathcal{Y}(\mathcal{Y}\log P(\mathcal{Y, X}, \bm{\theta})$ \begin{itemize} \item vs $\bm{\theta} \leftarrow \arg\max_{\bm{\theta}}\int d\mathcal{Y}Q_{\mathcal{Y}}(\mathcal{Y})\log P(\mathcal{Y, X}, \bm{\theta})$.  \end{itemize} \item VB reduces to EM if $Q_{\theta}(\bm{\theta})=\delta(\bm{\theta-\theta^*})$.  \item VB F increases monotonically, and incorporates the model complexity penalty. Since working with evidence directly, hope is that somehow it'd be robust to ovefitting. (TODO: how?) \end{itemize} 

Write down the F for variational Bayesian EM.; $F(Q_\mathcal{Y}, Q_{\bm{\theta}}) = \int \int d\mathcal{Y}d\bm{\theta}Q_{\mathcal{Y}}(\mathcal{Y})Q_\theta(\bm{\theta)}\log\frac{p(\mathcal{X, Y}, \bm{\theta}|\mathcal{M})}{Q_{\mathcal{Y}}(\mathcal{Y})Q_\theta(\bm{\theta})}}$

Write down the Q updates for Variational Bayesian EM; \begin{itemize} \item $Q^*_{\mathcal{Y}}(\mathcal{Y})\propto \exp \langle \log P(\mathcal{Y, X}|\bm{\theta})\rangle_{Q_\theta(\bm{\theta)}}$ \item $Q^*_{\theta}(\bm{\theta})\propto P(\bm{\theta})\exp \langle \log P(\mathcal{Y, X}|\bm{\theta})\rangle_{Q_\mathcal{Y}(\mathcal{Y})}$ \end{itemize}

Show that the variational Bayesian free energy is a lower bound of the log likelihood.; \begin{align*} \log P(\mathcal{X}) - F(Q_{\mathcal{Y}},Q_{\bm{\theta}}) &= \log P(\mathcal{X}) - \int \int d\mathcal{Y}d\bm{\theta}Q_{\mathcal{Y}}(\mathcal{Y})Q_\theta(\bm{\theta)}\log\frac{p(\mathcal{X, Y}, \bm{\theta}|\mathcal{M})}{Q_{\mathcal{Y}}(\mathcal{Y})Q_\theta(\bm{\theta})}} \\ &=  \int \int d\mathcal{Y}d\bm{\theta}Q_{\mathcal{Y}}(\mathcal{Y})Q_\theta(\bm{\theta)}\log\frac{Q_{\mathcal{Y}}(\mathcal{Y})Q_\theta(\bm{\theta})}{p(\mathcal{Y}, \bm{\theta}|\mathcal{X})}\\ &= KL(Q||P) \geq 0 \end{align*}

State the conditions for a latent variable model to be conjugate-exponential.; \begin{itemize} \item Joint probability over latent variables is in the exponential family \begin{itemize} \item $P(\mathcal{Z, X}|\bm{\theta}) = f(\mathcal{Z, X})g(\bm{\theta})\exp\{\bm{\phi(\theta)}^T\bm{T}(\mathcal{Z,X})\}$ \item $\bm{\phi(\theta)}$ is the vector of natural params \item $\bm{T}$ are sufficient statistics \item note marginal $P(\mathcal{X})$ is not necessarily in the exponential family (and usually isn't).  \end{itemize} \item Prior over parameters is conjugate to the joint probability \end{itemize}

Are the following models in the conjugate exponential family? If not, why not?  \begin{itemize} \item Gaussian mixtures \item factor analysis, PPCA \item Boltzmann machines, MRFs \item Logistic regression \item HMM, factorial HMM \item Linear dynamical systems, switching models \item Sigmoid belief networks \item Discrete-var belief networks (e.g. DAGs) \item Independent components analysis \end{itemize}; \begin{itemize} \item Gaussian mixtures \item factor analysis, PPCA \item Boltzmann machines, MRFs: no, no simple conjugacy, don't know how to normalise. but expfam.  \item Logistic regression: no simple conjugacy. nonlinearities linking latent at one step to ?? of next \item HMM, factorial HMM \item Linear dynamical systems, switching models \item Sigmoid belief networks: not exponential: nonlinearities linking latent at one step to ?? of next \item Discrete-var belief networks (e.g. DAGs) \item Independent components analysis: not exponential, nonlin implicitly in it \end{itemize} Note: one can often approx not-in-CE family models with a suitable choice from the CE family, e.g. MoGs.

Describe conjugate-exponential VB.; \begin{itemize} \item $Q_\theta(\bm{\theta})$ is also conjugate (to the log joint) \item $Q_\mathcal{Y}(\mathcal{Y}) = \prod_{i=1}^n Q_{\bm{y_i}}(\bm{y_i})$ takes the same form as in the E-step of regular EM.  \end{itemize}

Show that in Conjugate-exponential VB, $Q_\theta(\bm{\theta})$ is also conjugate.; TODO, L8 slide 26

Show the form $Q_\mathcal{Y}(\mathcal{Y})$ takes in conjugate-exponential VB.; TODO, L8 slide 26

What is the VB-E step doing?; \begin{itemize} \item Minimising the KL between $Q_\thetaQ_{\mathcal{Z}}$ and $Q(\mathcal{Z},\bm{\theta})$.  \end{itemize}

Can we use algorithms such as the junction tree, belief propagation and the Kalman filter in the VB-E step? If so, how?; Yes, using expected natural parameters.

What are the benefits and costs of using hierarchical models under an assumption that the Q distribution is factored? (L8 asides); \begin{itemize} \item Don't do more work even though model is hierarchical because of factored assumption \item More flexible: increases range of marginals you can model \item But decreased accuracy: Q is not actually factored \item Hoping resistant to overfitting because of complexity penalty \end{itemize}

How can you do model selection with VB? List two ways.; \begin{itemize} \item Hyperparameter selection (Hyper-M) step: $\bm{\eta} \leftarrow \arg\max_{\eta} F(Q_\mathcal{Z}, Q_\theta, \bm{\eta})$ \begin{itemize} \item $F(Q_\mathcal{Z}, Q_\theta, \bm{\eta}) = \int \int d\mathcal{Z}d\theta Q_{\mathcal{Z}}(\mathcal{Z})Q_\theta(\bm{\theta})\log\frac{P(\mathcal{X, Z},\bm{\theta}|\bm{\eta})}{Q_\mathcal{Z}(\mathcal{Z})Q_\theta(\bm{\theta})}$ \item Includes ARD for learning the latent dimensionality.  \end{itemize} \item Add to factored Q dist: $Q(\bm{eta})$. Blurs lines between latent variables and parameters.  \end{itemize}

What is ARD?; (Automatic relevance determination) \begin{itemize} \item Hyperparameter method to select relevant or useful inputs in regression.  \end{itemize} 

How can we learn a latent dimensionality in VB? (Example: factor analysis); \begin{itemize} \item Give the weights matrix a column-wise prior $\Lambda_i \sim N(0, \alpha_i^{-1}I)$.  \item So hyperparameter optimisation requires $\alpha \leftarrow \arg\max_{\alpha} F(Q_\mathcal{Z}, Q_\theta, \alpha) = \arg\max_\alpha \langle \log P(\Lambda |\alpha)\rangle_{Q_\Lambda}$.  \item $Q_\Lambda$ is Gaussian with the same form as in linreg, but with expected moments of y appearing in place of the inputs.  \item Opt wrt $\Psi$ (x noise) and $\alpha$ in turn cause some $\alpha_i$ to diverge (get v large) as in regression ARD. Those that diverge: irrelevant dimensions.  \end{itemize}

When might it be useful to introduce additional latent variables? Name two examples. (Augmented variational methods); \begin{itemize} \item To achieve computational tractability.  \item E.g. GP regression and GP latent var model.  \end{itemize}

% TODO: add slides on sparse GP approximations and variational sparse GP approximations (slide 33 onwards)

Formula for GP predictions given new datapoint $\mathbf{x'}$; $y'|X, Y, \mathbf{x'} \sim N(K_{\mathbf{x'}X}(K_{XX}+\sigma^2I)^{-1}Y, K_{\mathbf{x'x'}}-K_{\mathbf{x'}X}K_{XX+\sigma^2I}^{-1}K_{X\mathbf{x'}}+\sigma^2)$

Formula for GP evidence; $\log P(Y|X) = -\frac{1}{2}\log|2\pi(K_{XX}+\sigma^2I)|-\frac{1}{2}Y(K_{XX}+\sigma^2I)^{-1}Y^T$ 

Computational complexity of calculating GP predictions for new datapoint; \begin{itemize} \item If you haven't computed predictions before, $O(N^3)$ since you have to invert $K_{XX}$ (NxN). \item else much smaller \end{itemize}

What is the computational complexity of computing the evidence for GPs (for learning kernel hyperparameters)?; \begin{itemize} \item Evidence:  $\log P(Y|X)$ \item Need to invert $K_{XX}$ (NxN), so $O(N^3)$ time. \end{itemize}

How might we make computing GP predictions or evidence more efficient? (Describe the general approach); \begin{itemize} \item Find or select a smaller set of possibly fictitious measurements U (y) at inputs Z (x) such that  \item $P(y'|Z, U, \mathbf{x'}\approx P(y'|X, Y, \mathbf{x'})$. \item Q: how to choose U and Z? ($\rightarrow$ variational sparse GP approx) \end{itemize}

Describe how we can choose U and Z for sparse GP approximations; \begin{itemize} \item Variational sparse GP approx \item Write $F$ for the (smooth) GP function values that underlie Y (so $Y\sim N(F, \sigma^2I)$. \item Introduce latent measurements U at inputs Z (and integrate over U).  \item Both U and F are latent, so we introduce a variational dist $q(F, U)$ to form a free-energy. \item Choose var form $q(F, U)=P(F|U, X, Z)q(U)$. i.e. fix $F|U$ without reference to Y, so info about Y will need to be `compressed' into $q(U)$. Then \item $F(q(F, U), \theta, Z)=\langle \log \frac{P(Y|F)P(F|U,X,Z)P(U|Z)}{P(F|U,X,Z)q(U)}\rangle_{P(F|U)q(U)} = \langle \langle \log P(Y|F)\rangle_{P(F|U)} + \log P(U|Z) - \log q(U) \rangle_{q(U)}$. \item first term: only need Gaussian e(x) of LL since $P(F|U)$ is fixed by the generative model (vs subject to free opt). So can eval expectation under $P(F|U)$. \item combined expectation under $q(U)$ is the free energy of a PPCA-like model. Max of this free energy is the log-likelihood, achieved with q equal to the posterior under the PPCA-like model. \item (Final ans: $F(q^*(U), \theta, Z)=\log N(Y|0, K_{XZ}K^{-1}_{ZZ}K_{ZZ}K_{ZZ}^{-1}K_{ZX}+\sigma^2I) -\frac{1}{2\sigma^2}Tr[K_{XX}-K_{XZ}K_{ZZ}^{-1}K_{ZX}]$) \item Then optimise free energy numerically wrt Z, $\theta$ to adjust the GP prior and quality of var approx. \end{itemize}

Write down the likelihood in variational sparse GP approximations (after introducing latent measurements).; \begin{itemize} \item $P(Y|X)=\int \int dFdU P(Y, F, U|X, Z)$ \item $=\int int dFdU P(F|X)P(F|U, X, Z)P(U|Z)$ \item Sample U, then sample F|U. \item given X since GP specifies F given X. \end{itemize}

How might you interpret the latent U added at inputs Z in variational sparse GP approximations?; As augmenting the data. (We then integrate over U.)

What variational dist do we introduce in var sparse GP approx? Recall we have introduced latent measurements U at inputs Z.; \begin{itemize} \item Choose var form $q(F, U)=P(F|U, X, Z)q(U)$.  \item i.e. fix $F|U$ without reference to Y, so info about Y will need to be `compressed' into $q(U)$. \item $q(U)$ optionally Gaussian. \item (note not exactly factorisable) \end{itemize}

Time complexity of var sparse GP approx; $N\log N$ (TODO check, have some dodgy notes on s34).

Which latents do we marginalise over in variational sparse GP approx?; U only, not F. (todo check s34)

Interpret $\log P(U|Z)$ in variational sparse GP approximations. (U are the latents we're adding at inputs Z); Gaussian prior on U (todo check: from s35 notes)

How can you interpret this expectation? $\langle \log \frac{N(Y|K_{XZ}K_{ZZ}^{-1}U, \sigma^2I)P(U|Z)}{q(U)}\rangle_{q(U)}$ What is its maximum?; \begin{itemize} \item The free energy of a PPCA-like model with normal prior $U\sim N(0, K_{ZZ})$ and loading matrix $K_{XZ}K_{ZZ}^{-1}$. Similar bc: \begin{itemize} \item prior on y, \item latent Y, \item fixed rel between U and Y. \end{itemize} \item (and second term like PPCA-like cost?) \item The max of this free energy is the log-likelihood (achieved with $q$ equal to the posterior under the PPCA-like model). \end{itemize}

Evaluate $\langle \log P(Y|F)\rangle_{P(F|U)}$ where F are the smooth GP function values that underlie Y ($Y\sim N(F, \sigma^2I)$, and U are some latent measurements we introduce at inputs Z.; $\log N(Y|K_{XZ}K_{ZZ}^{-1}U, \sigma^2I)-\frac{1}{2\sigma^2}Tr[K_{XX}-K_{XZ}K_{ZZ}^{-1}K_{ZX}]$ \begin{itemize} \item (latter term not dependent on U, so outside of $q(U)$ expectation (which is not in the Q)) \item Steps: \item $\langle -\frac{1}{2}\log|2\pi\sigma^2I|-\frac{1}{2\sigma^2}Tr[(Y-F)(Y-F)^T]\rangle_{P(F|U)}$ \item $-\frac{1}{2}\log|2\pi\sigma^2I|-\frac{1}{2\sigma^2}Tr[(Y-\langle F\rangle_{P(F|U)})(Y-\langle F\rangle_{P(F|U)})^T]-\frac{1}{\sigma^2}Tr[\Sigma_{F|U}]$ \item last term: correlated uncertainty in Fs, not dep on obs or form of likelihood \end{itemize}

Give the final expression of the free energy when we want to find U and Z for sparse GP approximations.; (Opt this is a bit silly)  \begin{itemize} \item $F(q^*(U), \theta, Z)=\log N(Y|0, K_{XZ}K^{-1}_{ZZ}K_{ZZ}K_{ZZ}^{-1}K_{ZX}+\sigma^2I) -\frac{1}{2\sigma^2}Tr[K_{XX}-K_{XZ}K_{ZZ}^{-1}K_{ZX}]$ \item note that we've eliminated all terms in $K_{XX}^{-1}$. \item We can optimise the free energy numerically  wrt Z and $\theta$ to adjust the GP prior and quality of variational approximation. \end{itemize}

How can we learn X for GPs if they are unobserved, e.g. in the GPLVM?; \begin{itemize} \item Similar to variational sparse GP approx \item Assume $q(X, F, U)=q(X)P(F|X, U)q(U)$. \item Then $F=\langle \log P(Y, F, U|X)\log P(X)\rangle_{q(U)q(X)}$  \item which simplifies into tractable components in much the same way as above. \end{itemize}

\end{document}
