%Front;
% Lectured by Maneesh Sahani at the Gatsby Computational Neuroscience Unit, UCL, Fall 2018
% Link to course info and slides: http://www.gatsby.ucl.ac.uk/teaching/courses/ml1/
\documentclass{article}
\begin{document}

What does 'Markov' refer to (in Markov models)?; \begin{itemize} \item A conditional independence relationship.  \item Markov property: given the present observation, the future is independent of the past.  \end{itemize}

State one overall and three more specific goals of latent variable models for time series.; \begin{itemize} \item Gen: built joint probabilistiic model of the data $p(\mathbf{x_1,...x_n}$.  \item Predict $p(x_t|x_1,...,x_{t-1})$ \item Detect abnormal changed behaviour (if $p(x_t, x_{t+1}, ...|x_1,...,x_{t-1})$ small) \item Recover underlying / latent / hidden causes linking entire sequence.  \end{itemize}

Write an expression for the joint of latent chain (Markov) models.; \begin{itemize} \item $P(z_{1:T}, x_{1:T}) = P(z_1)P(x_1|z_1)\prod_{t=2}^TP(z_t|z_{t-1})P(x_t|z_t)$ \item $x$ and $z$ real-valued vectors.  \end{itemize} 

Name two frequently-used tractable latent chain models.; \begin{itemize} \item Linear Gaussian state-space models \item Hidden Markov models.  \end{itemize}

Describe the setup of a LGSSM; \begin{itemize} \item All conditional distributions are linear and gaussian.  \item Output eqn: $\mathbf{x_t}=C\mathbf{z}_t+\mathbf{v}_t$ \item State dynamics equation $\mathbf{z}_t=A\mathbf{z}_{t-1}+\mathbf{w}_t$ \item $\mathbf{v}_t \sim N(0,\sigma^2_v)$ \item $\mathbf{w}_t \sim N(0,\sigma^2_w)$ \item $\mathbf{z}_t$ is multivariate Gaussian. So joint $p(\mathbf{x}_{1:T}, \mathbf{z}_{1:T})$ is one big multivariate Gaussian.  \end{itemize}

Compare factor analysis and time-series state-space models.; \begin{itemize} \item Very similar, with $z_{t,j}$ (SSM) replacing $z_j$ in FA.  \item Sim: observations confined near low-dim subspace.  \item Diff: Successive observations generated from correlated points in the latent space (vs iid).  \item Diff: FA requires latent dim $K<D$ and $\Psi$ diagonal. SSM may have $K\geq D$ and arbitrary output noise. Because number of elements in covariance matrix (across time, TODO is x?) is much larger.  \item Thus ML estimates of subspace FA and SSM may differ.  \end{itemize}

Will ML estimates of subspaces (same K) necessarily be the same for FA vs LGSSMs?; \begin{itemize} \item No.  \item Diff: FA requires latent dim $K<D$ and $\Psi$ diagonal. SSM may have $K\geq D$ and arbitrary output noise. Because number of elements in covariance matrix (across time, TODO is x?) is much larger.  \item Thus ML estimates of subspace FA and SSM may differ. \item TODO: I don't think the answer matches the question \end{itemize} 

Name two ways of interpreting SSMs (i.e. linking them to other models).; \begin{itemize} \item Factor analysis where successive observations are generated from correlated points in the latent space.  \item Markov chain with linear (perturbed) dynamics in latents and linear projection to observations.  \end{itemize}

Describe the interpretation of SSMs relating to Markov chains.; \begin{itemize} \item Markov chain with linear dynamics in latents, \item Markov chain in latents perturbed by Gaussian innovations noise (may describe stochasticity, unknown control, or model mismatch.) \item Obs are a linear projection of the dynamical state, with additive iid Gaussian noise.  \end{itemize}

In a SSM, can the observations be of higher dimension than the latents? How about in factor analysis?; \begin{itemize} \item Yes for SSM, no for FA.  \end{itemize}

Write down an example of equations for a state space model with control inputs.; \begin{itemize} \item Inputs $u_t$, Outputs $x_t$.  \item State dynamics equation: $z_t=Az_{t-1}+Bu_{t-1}+w_t$ \item Output equation: $x_t=Cz_t+Du_t+v_t$ \end{itemize}

Describe the setup of Hidden Markov Models. (including equations); \begin{itemize} \item Discrete hidden state $s_t\in \{1,...,K\}$ \item Joint $p(s_{1:T}, \mathbf{x}_{1:T})=P(s_1)P(\mathbf{x}_1|s_1)\prod_{t=2}^TP(s_t|s_{t-1})P(\mathbf{x_t}|s_t)$ \item Initial state probs $\pi_j = P(s_1=j)$ \item Transition matrix $\Phi_{ij}=P(s_{t+1}=j|s_t=i)$ \item Emission (output) dists $A_i(\mathbf{x})=P(\mathbf{x}_t=\mathbf{x}|s_t=j)$ (continuous $\mathbf{x}_t$) \item or $A_{jk}=P(\mathbf{x}_t=k|s_t=j)$ for discrete $\mathbf{x}_t$.  \end{itemize}

Write down the joint for input-outut HMMs.; \begin{itemize} \item $p(s_{1:T}, \mathbf{x}_{1:T}|u_{1:T})=P(s_1|u_1)P(\mathbf{x}_1|s_1, u_1)\prod_{t=2}^TP(s_t|s_{t-1}, u_{t-1})P(\mathbf{x_t}|s_t, u_t)$ \end{itemize}

Briefly discuss the practicality and modelling capacity of input-output HMMs.; \begin{itemize} \item Can capture arbitrarily complex input-output relationships \item but number of states required is often impractical.  \end{itemize} 

Briefly discuss input-output HMMs.; \begin{itemize} \item Can capture arbitrarily complex input-output relationships \item But number of states required is often impractical.  \end{itemize}

State one advantage and one disadvantage of LGSSMs compared to HMMs.; \begin{itemize} \item Adv: Continuous vector state is a powerful representation: a real-valued state vector can store an arbitrary number of bits in principle (vs $2^N$ states to comm N bits of information for an HMM (since states discrete, i.e. bin).) \item Disadv: LG output/dynamics are very weak: can capture v limted dynamics. HMMs can in principle represent arbitrary stochastic dynamics and output mappings.  \end{itemize}

Name at least one HMM with richer state representation than a plain HMM.; \begin{itemize} \item Factorial HMMs \item Dynamic Bayesian networks. (TODO: what is this?) \end{itemize}

Do observations $x_t$ in a HMM need to be Markov?; No.

In a HMM, is $x_1$ independent of $x_3$ conditioned on $x_2$?; Not necessarily.

Why use the Markov assumption in the latents in HMMs?; \begin{itemize} \item Cond indep makes computation much easier.  \item Because data may be partially observed: e.g. classical physics local in time(?), working out what's really out there in the world \end{itemize}

Name two interpretations of HMMs; \begin{itemize} \item Markov chain with stochastic measurements (focus on latents $s_{1:T}$ \item Mixture model with states coupled over time ($s_t, x_t$ together) \end{itemize}

For an HMM, is the output process Markov of some order?; Not necessarily.

Can we use discrete state, discrete output models (HMMs) to approximate nonlinear continuous dynamics and observation mappings?; Yes, but this is usually not practical. 

TODO: what are stochastic finite state machines / automata?; (mentioned in a one-liner)

Briefly discuss the  information one (continuous) state can hold.; Depends on the noise magnitude. 

Compare the modelling capacity of HMM dynamics and linear Gaussian dynamics with respect to fixed points.; \begin{itemize} \item HMM can converge to $>1$ fixed points e.g. with cycles.  \item LGSSM can only converge to one or a line of fixed points.  \end{itemize}

Name one extension of HMMs; \begin{itemize} \item (*) Switching state space models \item Hierarchical models \item Constrained HMMs \item Continuous state models with discrete outputs for time series and static data \item (as long as you remember the first one it's fine) \end{itemize}

Describe the setup of Linear Gaussian SSMs. (including equations); \begin{itemize} \item Gaussian hidden state $z_t \sim N(\mathbf{\mu_0}, Q_0)$ \item Transition dynamics $\mathbf{z_t|z_{t-1}}\sim  N(A\mathbf{z_{t-1}}, Q)$ \item Emission (output) dists $\mathbf{x_t|z_t}\sim N(C\mathbf{z}_t, R)$.  \item Joint $P(\mathbf{x_1,...,x_T,z_1,...,z_T})=P(\mathbf{z_1})\prod_{t=2}^T P(\mathbf{z_t|z_{t-1}})\prod_{t=1}^T P(\mathbf{x_t|z_t})$ \end{itemize}

What information do we need when doing maximum likelihood learning with EM for LGSSMs?; \begin{itemize} \item Expectations needed in E-step are derived from singleton and pairwise marginals \item (Likewise only need singleton and pairwise expectations on q for M-step? TODO check) \item Since we can break down the log joint into pieces (since the joint has a factored structure.) \end{itemize}

Discuss how factored structure helps reduce computational cost in LGSSM models.; \begin{itemize} \item Don't need to invert matrix $(TK^3)$ cost (TODO: what matrix? Cov?) \item Only need subpieces \item And matrix has structure, can invert without explicit inversion. \item (from notes, slide 16 ML learning with EM) \end{itemize}

Name three general inference problems (and one more problem) in chain models; \begin{itemize} \item Filtering: $P(\mathbf{z_t|x_1,...,x_t})$ \item Smoothing: $P(\mathbf{z_t|x_1,...,x_T})$  \item (Also $P(\mathbf{z_t, z_{t-1}|x_1,...,x_T})$ for learning) \item Prediction: $P(\mathbf{z_t|x_1,...,x_{t-\Delta t}})$ \end{itemize}

Describe filtering; \begin{itemize} \item Identify distribution on latents using data up till current time \item  $P(\mathbf{z_t|x_1,...,x_t})$ \end{itemize}

Describe smoothing; \begin{itemize} \item Identify distribution of latent using all data. \item (Future information is informative about the past, e.g. given an airplane can't `jump', future radar data informative of where it is now.) \item $P(\mathbf{z_t|x_1,...,x_T})$ \end{itemize}

What integral may we have to calculate to evaluate $P(\mathbf{z_t|x_1,...,x_t})$? What makes us able to evaluate these in latent chain models?; \begin{itemize} \item $P(\mathbf{z_t|x_1,...,x_t}) = \int ...\int d\mathbf{z_1}...d\mathbf{z_{t-1}}P(\mathbf{z_1},...,\mathbf{z_t}|\mathbf{x_1},...,\mathbf{x_t}$ \item But the factored structure of the distributions will help us. \item The algorithms rely on a form of temporal updating or message passing. \end{itemize}

Describe a naive vs the dynamic programming way of finding $P(s_k|\mathbf{x_1,...,x_t})$.; \begin{itemize} \item Naive: start one bug at each state holding $P(s_1=m)(?)$, copy bug to each subsequent state, multiply by transition x output emission prob, sum all bugs per state at end timestep. (Rough description will do) \item FYI expression is $P(s_t=k|\mathbf{x_1,...,x_t})=\sum_{k_1, ..., k_{t-1}}P(s_1=k_1,...,s_t=k|\mathbf{x_1,...,x_t}\propto\sum_{k_1,...,k_{t-1}}\pi_{k_1}A_{k_1}\mathbf{x_1}\phi_{k_1,k_2}A_{k_2}(\mathbf{x_2})...\phi_{k_{t-1},k}A_k(\mathbf{x_t})$ \item Recursive (DP): at every timestep, replace bugs at each node with a single bug carrying the sum of values. \end{itemize}

Write down/derive the recursive formula for calculating $P(\mathbf{z_t}|x_{1:t})$; \begin{itemize} \item $P(\mathbf{z_t}|x_{1:t})$ \item $=\int P(\mathbf{z_t, z_{t-1}, x_t |x_{1:t-1}})d\mathbf{z_{t-1}}$ \item $=\int \frac{P(\mathbf{z_t, z_{t-1}|x_t, x_{1:t-1}})}{P(\mathbf{x_t|x_{1:t-1}})}d\mathbf{z_{t-1}}$ (Bayes' Rule) \item $\propto\int P(\mathbf{x_t| z_t, z_{t-1}, x_{1:t-1}})P(\mathbf{z_t|z_{t-1}, x_{1:t-1}}) P(\mathbf{z_{t-1}|x_{1:t-1}}) d\mathbf{z_{t-1}}$ \begin{itemize} \item Factor out $z_t, z_{t-1}$ \item consider only numerator \end{itemize} \item $=\int P(\mathbf{x_t| z_t})P(\mathbf{z_t|z_{t-1}}) P(\mathbf{z_{t-1}|x_{1:t-1}}) d\mathbf{z_{t-1}}$ \begin{itemize} \item First term: $x_t$ conditionally independent of $z_{t-1}, x_{1:t-1}$ given $z_t$ \item second term: $z_t$ cond indep of $x_{1:t-1}$ given $y_{t-1}$. \end{itemize} \item This is a forward recursion based on Bayes' rule.  \item (Complexity of update depends on dim of state and dists involved.) \end{itemize}

Write down the formulae for the HMM forward pass and state the time complexity.; \begin{itemize} \item Define $\alpha_t(i)=P(\mathbf{x_1,...,x_t}, s_t=i|\theta)$ \begin{itemize} \item Note: this is a joint vs a posterior. Can obtain posterior by normalising (divide by $\sum_k \alpha_t(k)$.) \end{itemize} \item then $\alpha_1(i)=P(s_1=i)P(x_1|s_1=i)=\pi_iA_i(\mathbf{x_1})$, \item $\alpha_{t+1}(i)=\big(\sum_{j=1}^K\alpha_t(j)\Phi_{ji}\big)A_i(\mathbf{x}_{t+1})$. \begin{itemize} \item $\Phi_{ji}=P(s_{t+1}=i|s_t=j)$ \item i.e. term in brackets is $ P(s_{t+1}=i, \mathbf{x_1,...,x_t}|\theta)$. \item $A_i(\mathbf{x}_{t+1})=P(x_{t+1}|s_{t+1}=i)$. \end{itemize} \item Compute likelihood: $P(\mathbf{x_1,...,x_T}|\theta)=\sum_{s_1,...,s_T}P(\mathbf{x_1,...,x_T}, s_1,...,s_T,\theta)=\sum_{k=1}^K\alpha_T(k)$. \begin{itemize} \item $=\sum_{k=1}^KP(s_T=k, \mathbf{x_1,...,x_T}|\theta)$ \end{itemize} \item Time complexity: $O(TK^2)$ vs naive sum $O(K^T)$ \end{itemize}

Briefly describe Kalman filtering (TODO check when finished going through L4); \begin{itemize} \item (1) Propose new belief about state $P(\mathbf{y_t|x_{1:t-1}})$. \item (2) Incorporate new datapoint $P(\mathbf{y_t|x_{1:t}})$. \item Preds look at prev prediction error, error affects est of state mean with some scaling (Kalman gain). \item Model: \begin{itemize} \item MSE est of state  \begin{itemize} \item Corr between Gaussian p(0) dist and MSE bc likelihood of Gaussian is squared error \end{itemize} \item where state process is linear,  \begin{itemize} \item (Correct posterior beliefs for Fourier?) \end{itemize} \item without specifying the noise \end{itemize}  \end{itemize}

State the equation for $P(\mathbf{y_1|x_1})$ for Kalman Filtering (LGSSM); \begin{itemize} \item Model: \begin{itemize} \item $\mathbf{y_1}\sim N(\mathbf{\mu_0},Q_0)$ \item $\mathbf{y_t|y_{t-1}}\sim N(A\mathbf{y}_{t-1}, Q)$ \item $\mathbf{x_t|y_t}\sim N(C\mathbf{y_t},R)$ \end{itemize} \item Let $\hat{\mathbf{y}}^0_1=\mathbf{\mu_0}$ and $\hat{V}^0_1=Q_0$. Then \begin{itemize} \item superscript: given data, subscript: timestep you're estimating \item Note V is posterior variance, not est var \end{itemize} \item $P(\mathbf{y_1|x_1})=N(\mathbf{\hat{y}_1^0+K_1(\mathbf{x_1}-C\hat{\mathbf{y}}^0_1}), \hat{V^0_1}-K_1C\hat{V}^0_1)=N(\mathbf{\hat{y}^1_1}, \hat{V}^1_1)$. \begin{itemize} \item Mean: mean in y space + (K: beta of y on x) $\times$ (deviation from mean in x). \item Var: TODO \item C maps y to x space. \item Kalman gain $K_1=\hat{V}^0_1C^T(C\hat{V}^0_1C^T+R)^{-1}$ \end{itemize} \end{itemize}
    
State the general update equations for Kalman Filtering (LGSSM); \begin{itemize} \item In general, we define $\hat{\mathbf{y}}^\tau_t \equiv E[\mathbf{y_t|x_1,...,x_\tau}]$ and $\hat{V}^\tau_t \equiv V[\mathbf{y_t}|x_1,...,x_\tau]$. Then \item (1) Propose new belief about state $P(\mathbf{y_t|x_{1:t-1}})=\int d\mathbf{y}_{t-1}P(\mathbf{y_t|y_{t-1}})P(\mathbf{y_{t-1}|x_{1:t-1}})=N(A\hat{y}^{t-1}_{t-1}, A\hat{V}^{t-1}_{t-1}A^T+Q)=N(\hat{\mathbf{y}}^{t-1}_t, \hat{V}^{t-1}_t)$. \item (2) Incorporate new datapoint $P(\mathbf{y_t|x_{1:t}})=N(\hat{\mathbf{y}}^{t-1}_t+K_t(\mathbf{x_t}-C\mathbf{\hat{y}}^{t-1}_t), \hat{V}^{t-1}_t - K_tC\hat{V}^{t-1}_t)=N(\mathbf{\hat{y}}^t_t, \hat{V}^t_t)$. \begin{itemize} \item Kalman gain $K_t = \hat{V}^{t-1}_tC^T(C\hat{V}^{t-1}_tC^T+R)^{-1} = \langle \mathbf{yx}^T \rangle \langle \mathbf{xx^T} \rangle^{-1}$ \item Preds look at prev prediction error, error affects est of state mean with some scaling (Kalman gain) \end{itemize} \begin{itemize} \item superscript: given data, subscript: timestep you're estimating \item Note V is posterior var, not est var \end{itemize} \item Model: \begin{itemize} \item $\mathbf{y_1}\sim N(\mathbf{\mu_0},Q_0)$ \item $\mathbf{y_t|y_{t-1}}\sim N(A\mathbf{y}_{t-1}, Q)$ \item $\mathbf{x_t|y_t}\sim N(C\mathbf{y_t},R)$ \end{itemize} \end{itemize}

What is innovations noise in a LGSSM?; \begin{itemize} \item Noise in latent state transition \item i.e. Q in $\mathbf{y_t|y_{t-1}}\sim N(A\mathbf{y_{t-1}, Q})$. \end{itemize}

Give two motivations for `Bayesian smoothing', i.e. finding the marginal posterior $P(\mathbf{y_t|x_{1:T}})$.; \begin{itemize} \item Finding the entire trajectory of a plane given radar data \item Learning \end{itemize}

Write down the general formula to find the marginal posterior $P(\mathbf{y_t|x_{1:T}})$ via Bayesian smoothing; \begin{itemize} \item $\frac{P(\mathbf{y_t, x_{t+1:T}|x_{1:t}})}{P(\mathbf{x_{t+1:T}|x_{1:t}})}$ \item $=\frac{P(\mathbf{x_{t+1:T}|y_t})P(\mathbf{y_t|x_{1:t}})}{P(\mathbf{x_{t+1:T}|x_{1:t}})}$ \item num = (bwkd x fwd). Split out $y_t$. \end{itemize}

Describe the HMM forward-backward algorithm for smoothing; \begin{itemize} \item State estimation: $\gamma_t(i)\equiv P(s_t=i|\mathbf{x_{1:T}})$ \item $=\frac{P(s_t=i, \mathbf{x_{1:t}})P(\mathbf{x_{t+1:T}|s_t=i}}{P(\mathbf{x_{1:T}}}$ \begin{itemize} \item Trick: breaking up x by time \item num = fwd x bkwd \end{itemize} \item $=\frac{alpha_t(i)\beta_t(i)}{\sum_j\alpha_t(j)\beta_t(j)}$ \item forward and backward recursions for $\alpha, \beta$ \item $\alpha_t(i)$ gives total inflow of probabilities to node (t, i), $\beta_t(i)$ gives total outflow of probabilities \end{itemize}

State / derive the backward recursion formula for $\beta_t(i)$ in the HMM forward-backward algorithm.; \begin{itemize} \item $\beta_t(i)=P(\mathbf{x_{t+1:T}|s_t=i})$ \item $=\sum_{j=1}^K P(s_{t+1}=j, \mathbf{x_{t+1},  x_{t+2:T}}|s_t=i)$ \item = $\sum_{j=1}^K P(s_{t+1}=j, s_t=i) P(\mathbf{x_{t+1}}|s_{t+1}=j)P(\mathbf{x_{t+2:T}}|s_{t+1}=j)$ \item $=\sum_{j=1}^K\Phi_{ij}A_j(\mathbf{x_{t+1}})\beta_{t+1}(j)$. \end{itemize}

Does the forward-backward algorithm give the path with the highest probability of generating the data?; \begin{itemize} \item No.  \item It gives the path with the maximum expected number of correct states. \item i.e. grade timepoint by timepoint which state has highest probability vs a single continuous path. \item Viterbi finds most probable state sequence. $\arg\max_{s_{1:T}}P(s_{1:T}|\mathbf{x_{1:T},\theta})$ \end{itemize}

Describe Viterbi decoding; \begin{itemize} \item max-product $\arg\max_{s_{1:T}}P(s_{1:T}|\mathbf{x_{1:T},\theta})$  \item (vs fwd-bkwd sum-product) \item Computes most probably state sequence \item (bug-wise: at each step, kill all bugs except for the one with the highest value at the node. \end{itemize}

TODO: Describe modified EM training based ont he Viterbi decoder and what it's good for; \begin{itemize} \item Act as though most likely path was an observed path (in practice, not in principle) \item Good bc for some models it's easier to find the max than to find the full posterior marginals \end{itemize}

Compare the forward-backward algorithm and Viterbi decoding in one sentence.; \begin{itemize} \item forward-backward: Sum-product $\arg\max_{i}P(s_t=i|\mathbf{x}_{1:T})$ vs \item Viterbi: max-product $\arg\max_{s_{1:T}}P(s_{1:T}|\mathbf{x_{1:T},\theta})$ \end{itemize}

Write down the formulae for Kalman Smoothing for the LGSSM (general x and y, no need for matrices); \begin{itemize} \item $P(\mathbf{y_t|x_{1:T}})=\int P(\mathbf{y_t, y_{t+1}|x_{1:T}}d\mathbf{y_{t+1}}$ \item $=\int P(\mathbf{y_t|y_{t+1},x_{1:T}}P(\mathbf{y_{t+1}|x_{1:T}}) d\mathbf{y_{t+1}}$ \item $=\int P(\mathbf{y_t|y_{t+1},x_{1:t}}P(\mathbf{y_{t+1}|x_{1:T}}) d\mathbf{y_{t+1}}$ \begin{itemize} \item by the Markov property. \end{itemize} \end{itemize}

Compare (in words) the smoothing algorithms for HMMs (fwd-bkwd) vs the LGSSM; \begin{itemize} \item HMM: calculate forward, backward and multiply together at the end \item LGSSM: don't bring future xs to other side (TODO?) \begin{itemize} \item  $P(\mathbf{y_t|x_{1:T}})=\int P(\mathbf{y_t|y_{t+1},x_{1:t}}P(\mathbf{y_{t+1}|x_{1:T}}) d\mathbf{y_{t+1}}$ \end{itemize} \end{itemize}

State the matrices for backward recursion for Kalman smoothing; TODO how use? guessing $N(\mathbf{\hat{y}^T_t},\hat{V}^T_t)$?  \begin{itemize} \item $J_t=\hat{V}^t_tA^T(\hat{V}^t_{t+1})^{-1}$ \begin{itemize} \item Like regression coefficient \item A maps $y_t$ to $y_{t+1}$ \end{itemize} \item $\mathbf{\hat{y}^T_t}=\hat{\mathbf{y}}^t_t+J_t(\hat{\mathbf{y}}^T_{t+1}-A\hat{\mathbf{y}}^t_t)$ \begin{itemize} \item Mean + regr coeff * (deviation?) \end{itemize} \item $\hat{V}^T_t=\hat{V}^t_t+J_t(\hat{V}^T_{t+1}-\hat{V}^t_{t+1})J_t^T$ \end{itemize}

Describe how you'd do maximum likelihood learning for SSMs (LGSSM) using batch EM.; \begin{itemize} \item E-step (max F wrt q with $\theta$ fixed): two-state extension of Kalman smoother $q^*(\mathbf{y})=p(\mathbf{y|x}, \theta)$ \item M-step: solving a few weighted least squares problems, since all the variables in $p(\mathbf{y, x}|\theta)=p(\mathbf{y_1})p(\mathbf{x_1|y_1})\prod_{t=2}^Tp(\mathbf{y_t|y_{t-1}})p(\mathbf{x_t|y_t})$ form a multivariate Gaussian. \item FYI difficulty: E-step usually harder, M-step usually easier (usually standard exponential family updates) \end{itemize}

Derive the M-step for C (maps $y_t$ to $x_t$) in the LGSSM.; (TODO expand question? not clear?) \begin{itemize} \item TODO  \end{itemize}

Derive the M-step for A (maps $y_t$ to $y_{t+1}$) in the LGSSM; TODO expand q not clear wt M-step is for?
% Bayesian Smoothing slide 27

How do the M-steps for C (maps $y_t$ to $x_t$) and A (maps $y_t$ to $y_{t+1}$) in the LGSSM model compare to factor analysis?; \begin{itemize} \item C: same as (WHAT) in factor analysis \item A: analogous to factor analysis, with expected correlations (FA has what instead?) TODO \end{itemize}

Describe how you might update the parameters of an LGSSM model online as observations arrive.; \begin{itemize} \item Differentiate conditional log likelihoods at each timestep $l_t=\ln p(\mathbf{x_t|x_1,...x_{t-1}})$ to obtain gradient rules for parameters $A, C, Q, R$ \item Note $l=\sum_{t=1}^T\ln  p(\mathbf{x_t|x_1,...x_{t-1}})=\sum_{t=1}^Tl_t$. \item An approximate version of the filtering version of q (in notes, haven't verified). Unclear what this mismatch means. \item Interpretation (from notes): instead of just max likelihood, maximising additive terms (one at a time?), may be advantageous when dynamics are nonstationary (why?) \item Size of the gradient step (lr) reflects our expectation about nonstationarity (more nonstationary, smaller step?) \end{itemize}

Describe how you'd learn HMMs using EM and state what it's called.; \begin{itemize} \item Baum-Welch \item E-step: $q^*(s_{1:T})=P(s_{1:T}|\mathbf{x_{1...?T}}, \theta)$ \begin{itemize} \item Only need marginal probabilities $q(s_t, s_{t+1})$, which can also be obtained from the forward-backward algorithm. \end{itemize} \item M-step: Re-estimate the parameters by computing the expected number of times the HMM was in state i, emitted symbol k and transitioned to state j. (TODO?) \end{itemize}

State the M-step updates for HMM.; \begin{itemize} \item $\hat{\pi}_i = \gamma_1(i) = P(s_t=i|x_{1:T})$ \item Expected number of transitions from state i to j which begin at time t \begin{itemize} \item $\xi_t(i\rightarrow j) \equiv P(s_t=i, s_{t+1}=j|\mathbf{x_{1:T}})$ \item $=\alpha_t(i)\Phi_{ij}A_j(\mathbf{x_{t+1}})\beta_{t 1}(j)/P(x_{1:T})$ \end{itemize} \item So the estimated transition probabilities are \begin{itemize} \item $\hat{\Phi_{ij}}=\sum_{t=1}^{T-1}\xi_t(i\rightarrow j)/\sum_{t=1}^{T-1}\gamma_t(i)$ \end{itemize} \item Output dists are expected number of times we observe a particular symbol in a particular state: \begin{itemize} \item $\hat{A}_{ik}=\sum_{t:\mathbf{x}_t=k}\gamma_t(i)\big / \sum_{t=1}^T\gamma_t(i)$ \item = number of times made obs k and state was i (joint) div by (prob state was i) \item (or the state-probability weighted mean and variance for a Gaussian output model) \end{itemize} \end{itemize}

Describe a practical thing to do when doing EM for HMMs.; Numerical scaling \begin{itemize} \item The conventional message definition is in terms of a large joint: $\alpha_t(i)=P(\mathbf{x_{1:t}},s_t=i)\rightarrow 0$ as t grows, and so can easily underflow. \item Rescale: $\bar{\alpha}_t=A_i(\mathbf{x_t})\sum_j\tilde{\alpha}_{t-1}(j)\Phi_{ji}$, \begin{itemize} \item $\rho_t = \sum_{i=1}^K\bar{\alpha}_t(i)$ \item $\tilde{alpha}_t(i)=\bar{\alpha}_t(i)/\rho_t$ \item Can show that $\rho_t=P(\mathbf{x_t|x_{1:t-1}}, \theta)$ \item and $\prod_{t=1}^T\rho_t = P(\mathbf{x_{1:T}}|\theta)$ \item TODO: what does this make $\tilde{\alpha}_t(i)$? \end{itemize} \item Can also represent as log \end{itemize} Other issues: \begin{itemize} \item Multiple observed sequences (can average num and denoms in the ratios of updates. Be careful of seqs with diff lengths: should not give them equal weights. Should be sum across transitions of diff seqs (?)) \item Local optima (use random resstarts, annealing, more on this later) \end{itemize}

Write pseudocode for the E-step for an HMM; TODO slide 38 (didn't go through this in lecture)

Write pseudocode for the M-step for an HMM (Baum-welch); TODO slide 39 (didn't go through this in lecture)

What kind of transform of the latent conserves the LGSSM likelihood?; Any invertible transform of the latent. 

Show that any invertible transform of the latent conserves the LGSSM likelihood.; \begin{itemize} \item Have \begin{itemize} \item $P(\mathbf{y_{t+1}|y_t})=N(A\mathbf{y_t}, Q)$ \item $P(\mathbf{x_{t}|y_t})=N(C\mathbf{y}, R)$  \end{itemize} \item transform $\tilde{\mathbf{y}}=G\mathbf{y}$ \item $\tilde{Q}=GQG^T, \tilde{A}=GAG^{-1}, \tilde{C}=CG^{-1}$ \item so we have $P(\mathbf{\tilde{y}_{t+1}|\tilde{y}_t})=N(GAG^{-1}G\mathbf{y}_t, GQG^T)=N(\tilde{A}\tilde{\mathbf{y}}_t, \tilde{Q})$ \item and $P(\mathbf{x}_t, \mathbf{\tilde{y}}_t)=N(CG^{-1}G\mathbf{y}, R)=N(\tilde{C}\tilde{\mathbf{y}}, R)$ \end{itemize}

What kinds of transform of the latent conserves the FA likelihood?; Orthogonal transformations of the latent $\mathbf{y}$. TODO: expand

What kinds of transform of the latent conserves the mixture model likelihood?; Permutations of the latent.

What kinds of transform of the states is the HMM likelihood invariant to?; \begin{itemize} \item Permutations of the states \item (and to relaxations into something called an observable operator model, wtv bc dk what this is) \end{itemize}

Do LGSSM and HMM likelihood functions tend to have a single or multiple local maxima?; \begin{itemize} \item Multiple local maxima \end{itemize}

Should we worry much if the parameters of a model are not identifiable?; (Opinion-ish q, it depends) Not necessarily - if we need to identify the parameters exactly, we'd presumably have a loss function to select a soluttion. Generally we just fand to find a good set of parameters.

What strategies might one use to deal with the fact that LGSSM and HMM likelihood functions tend to have multiple local maxima? (x5); \begin{itemize} \item Restart EM/gradient ascent from different (often random) initial param values. $\rightarrow$ likely to find diff results. \item Initialise output params with a stationary model and go from there. E.g. PCA to FA to LGSSM, or k-means to mixture model to HMM. \item Stochastic gradient methods and momentum (to avoid shallow local optima), `overstepping' EM. (M-step stepping past max) \item Deterministic annealing (what is this? TODO I wrote sth about doing this vs true posterior may use higher temp?) \item Non-ML learning (spectral methods) \end{itemize}

Describe slow feature analysis (SFA); \begin{itemize} \item Zero-noise limit for LGSSM analogous to PCA. \item Params: \begin{itemize} \item $A=diag[a_1,...,a_k]\rightarrow I, a_1\leq a_2 \leq ...\leq a_K\leq 1$ \item $Q=I-AA^T\rightarrow 0$ (set stationary latent cov to $I$) \item $R\rightarrow 0$ \end{itemize} \item Conventionally defined as slowness pursuit: given zero-mean series $\{\mathbf{x_1,...,x_T}\}$, find a $K\times D$ matrix $W(=C^T)$ such that $\mathbf{y}_T=W\mathbf{x}_t$ changes as slowly as possible. \begin{itemize} \item Specifically, find $W=\arg\min_W\sum_t||\mathbf{y}_t-\mathbf{y}_{t-1}||^2$ subject to $\sum_t\mathbf{y_ty_t^T}=TI$. \item i.e. make sure $yy^T$ conveys info about $x_t$? Or that W is var-preserving in some sense (TODO check) \end{itemize} \end{itemize}

Describe slowness pursuit.; \begin{itemize} \item Given zero-mean series $\{\mathbf{x_1,...,x_T}\}$, find a $K\times D$ matrix $W(=C^T)$ such that $\mathbf{y}_T=W\mathbf{x}_t$ changes as slowly as possible. given zero-mean series $\{\mathbf{x_1,...,x_T}\}$, find a $K\times D$ matrix $W(=C^T)$ such that $\mathbf{y}_T=W\mathbf{x}_t$ changes as slowly as possible. \item Specifically, find $W=\arg\min_W\sum_t||\mathbf{y}_t-\mathbf{y}_{t-1}||^2$ subject to $\sum_t\mathbf{y_ty_t^T}=TI$. \begin{itemize} \item i.e. make sure $yy^T$ conveys info about $x_t$? Or that W is var-preserving in some sense (TODO check) \item Var constraint prevents trivial solutions $W=0$, $W=\mathbf{1w}^T$. \end{itemize} \end{itemize}

How do you find the W in slowness pursuit?; (likely not important to know) \begin{itemize} \item By solving the generalised eigenvalue problem $WA=\Omega WB$,  \item where $A=\sum_t (\mathbf{x}_t - \mathbf{x}_{t-1})(\mathbf{x_t - x_{t-1}})^T$ (sq of diff of x) and $B=\sum_t\mathbf{x_tx_t^T}$ (var on x) \end{itemize} 

% TODO: slide 42 side notes on RHS

Describe general MoM estimators and their relationship with maximum likelihood estimation.; \begin{itemize} \item Estimates $\theta^*$ with \item $\langle T(\mathbf{x}) \rangle_{\theta^*} = \frac{1}{N}\sum_i T(\mathbf{x_i})$ \item or $\arg\min_\theta ||\langle T(\mathbf{x})\rangle_\theta - \frac{1}{N}\sum_i T(\mathbf{x}_i)||_\mathcal{C}$ \begin{itemize} \item arbitrary cost C \end{itemize} \item Judicious choice of T and metric $\mathcal{C}$ might make the solution unique (no local optima) and consistent (correct given infinite within-model data). \item From: $\theta^*=\theta^{ML}$ if $P$ is in the exponential family. So casts $\theta^{ML}$ as a kind of moment-matching pursuit within the exponential family. \end{itemize}

Describe Ho-Kalman SSID for LGSSMs; TODO (have equations later)

State the equations for Ho-Kalman SSID for LGSSMs; \begin{itemize} \item $M_\tau \equiv \langle \mathbf{x_{t+\tau}x^T_t}=\langle \mathbf{x}_{t+\tau}\mathbf{y}^T_t \langle C^T$ \item $=CA^\tau \langle \mathbf{y_ty_t^T}\rangle C^T=CA^\tau \Pi C^T$ \begin{itemize} \item Lagged covariance \end{itemize} \item $H\equiv \langle \mathbf{x_t^+ x_t^{-T}}=\big [ M_1\text{ to }M_L\text{in each row and col} \big]$ $= [C CA ... CA^{L-1}]^T [A\Pi C^T...A^L\Pi C^T] = \Xi\Upsilon$ \begin{itemize} \item Matrix of Ms is $LD\times LD$, row i is $M_i, M_{i+1},...,M_{L+i-1}$. \item $\Xi$ is $LD\times K$ \item $\Upsilon$ is $K\times LD$. \item $\mathbf{x}^+_t = [\mathbf{x}_{t:t+L-1}]$, $\mathbf{x}_t^- = [\mathbf{x}_{t-1:t-L}]$. \end{itemize} \item Off-diagonal correlation unaffected by noise. \item $SVD(\frac{1}{T}\sum\mathbf{x}_t^+\mathbf{x}_t^{-T})$ yields least-squares estimates of $\Xi, \Gamma$. \item Regression between blocks of $\Xi$ yields $\hat{A}, \hat{C}$. \item TODO: process properly and understand slide (44) \end{itemize}

Describe the observable operator model (OOM) representation; \begin{itemize} \item from HMMs with discrete output symbols \item Rewrite likelihood as $P(x_{1:T}|\pi, \mathbf{\Phi}, A)=\sum_i[\pi_iA_i(x_1)\sum_j\Phi{ji}A_j(x_2)\sum_k\Phi_{kj}A_K(x_3)...$ \begin{itemize} \item $=\mathbf{\pi}^TA_{x_1}\Phi^TA_{x_2}\Phi^T A_{x_3}...\mathbf{1}$ [$A_{x_1}=diag[A_1(x_1), A_2(x_1),...]]$ \item $=\mathbf{1^T}O_{x_T}O_{x_{T-1}}...O_{x_1}\mathbf{\pi}$, \item where $O_a = \PhiA_a$ is a `propagation operator' on the latent belief that depends on observation. \end{itemize} \item Notes \begin{itemize} \item OOMs with arbitrary O matrices describe a larger class of distributions than HMMs \item Not easy to normalise or even guarantee all assigned `probabilities' are positive \item Degenerate wrt similarity transform $\tilde{O}=GOG^{-1}$. (similar to LGSSM in terms of degeneracy). \end{itemize} \end{itemize}

Describe spectral learning for HMMs (no need equations, stated no need to learn to reproduce); \begin{itemize} \item Derivation similar to Ho-kalman SSID \item Instead of obs being $\mathbf{x}$, treat $\mathbf{x}$ as discrete, represent as a one-hot vector \item TODO: add brief description of ho-kalman SSID. \end{itemize}

Is it possible to project to HMM space from a spectral learning HMM?; \begin{itemize} \item Yes, but this amplifies estimation errors. (something on recount estimates correlated to valued(?) $\Phi, A$.) \end{itemize} 

What is spectral learning?; \begin{itemize} \item Have a big matrix, take the SVD. \end{itemize}

Describe the pros and cons of spectral learning (vs maximum likelihood); \begin{itemize} \item Pros \begin{itemize} \item Efficient closed-form solution finds global optimum / unique soln. \item Consistent (recovers true params up to degeneracies from infinite within-model data) \item Eigen/singular value spectrum clue to latent dim \item Often valuable as initialisation for max likelihood methods. (e.g Ho-Kalman as init for EM, since spectral methods avoid many pitfalls of local optima.) \end{itemize} \item Cons \begin{itemize} \item Assumes stationarity, may be inappropriate for short sequences \item HMM learning returns OOM parameters: may not correspond to any HMM or indeed to a proper probabilistic model \item Not easily generalised to non-linear or more complex models \item In practice, error in recovered parameters is often large.  \item Not data efficient, especially relative to max likelihood. \end{itemize} \end{itemize}

Describe the pros and cons of maximum likelihood learning; \begin{itemize} \item Pros: \begin{itemize} \item Consistent (caveat next line) \item Asymptotically efficient (if the global maximum can be found) \item Generalises to `principled' approx algorithms for nonlinear or complex models  \end{itemize} \item Cons \begin{itemize} \item Requires iterative maximisation \item Many local optima \end{itemize} \end{itemize}

How do you perform classification with HMMs?; \begin{itemize} \item Have multiple HMM models, one for each class (requires each seq to have a class label) \item Eval probability of unknown sequence (being a certain class) under each HMM \item Classify the unknown seq by the HMM which gave it the highest likelihood \item Or single HMM: compute posterior over label sequences (labels as s), return label seq either with the highest expected number of correct states or highest probability under the posterior (Viterbi, entire seq) \begin{itemize} \item Note here we model whole joint but only use $P(s_{1:T}|x_{1:T})$ - can model that directly using a conditional random field (CRF). \end{itemize} \end{itemize}

% TODO: slide 50 onwards, not sure if we covered this.

\end{document}
