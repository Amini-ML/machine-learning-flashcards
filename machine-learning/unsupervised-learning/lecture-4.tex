%Front; Back
\documentclass{article}
\begin{document}

What does 'Markov' refer to (in Markov models)?; \begin{itemize} \item A conditional independence relationship.  \item Markov property: given the present observation, the future is independent of the past.  \end{itemize}

State one overall and three more specific goals of latent variable models for time series.; \begin{itemize} \item Gen: built joint probabilistiic model of the data $p(\mathbf{x_1,...x_n}$.  \item Predict $p(x_t|x_1,...,x_{t-1})$ \item Detect abnormal changed behaviour (if $p(x_t, x_{t+1}, ...|x_1,...,x_{t-1})$ small) \item Recover underlying / latent / hidden causes linking entire sequence.  \end{itemize}

Write an expression for the joint of latent chain (Markov) models.; \begin{itemize} \item $P(z_{1:T}, x_{1:T}) = P(z_t)P(x_t|z_t)\prod_{t=2}^TP(z_t|z_{t-1})P(x_t|z_t)$ \item $x$ and $z$ real-valued vectors.  \end{itemize} 

Name two frequently-used tractable latent chain models.; \begin{itemize} \item Linear Gaussian state-space models \item Hidden Markov models.  \end{itemize}

Describe the setup of a LGSSM; \begin{itemize} \item All conditional distributions are linear and gaussian.  \item Output eqn: $\mathbf{x_t}=C\mathbf{z}_t+\mathbf{v}_t$ \item State dynamics equation $\mathbf{z}_t=A\mathbf{z}_{t-1}+\mathbf{w}_t$ \item $\mathbf{v}_t \sim N(0,\sigma^2_v)$ \item $\mathbf{w}_t \sim N(0,\sigma^2_w)$ \item $\mathbf{z}_t$ is multivariate Gaussian. So joint $p(\mathbf{x}_{1:T}, \mathbf{z}_{1:T})$ is one big multivariate Gaussian.  \end{itemize}

Compare factor analysis and time-series state-space models.; \begin{itemize} \item Very similar, with $z_{t,j}$ (SSM) replacing $z_j$ in FA.  \item Sim: observations confined near low-dim subspace.  \item Diff: Successive observations generated from correlated points in the latent space (vs iid).  \item Diff: FA requires latent dim $K<D$ and $\Psi$ diagonal. SSM may have $K\geq D$ and arbitrary output noise. Because number of elements in covariance matrix (across time, TODO is x?) is much larger.  \item Thus ML estimates of subspace FA and SSM may differ.  \end{itemize}

Will ML estimates of subspaces (same K) necessarily be the same?; \begin{itemize} \item No.  \item Diff: FA requires latent dim $K<D$ and $\Psi$ diagonal. SSM may have $K\geq D$ and arbitrary output noise. Because number of elements in covariance matrix (across time, TODO is x?) is much larger.  \item Thus ML estimates of subspace FA and SSM may differ.  \end{itemize} 

Name two ways of interpreting SSMs (i.e. linking them to other models).; \begin{itemize} \item Factor analysis where successive observations are generated from correlated points in the latent space.  \item Markov chain with linear (perturbed) dynamics in latents and linear projection to observations.  \end{itemize}

Describe the interpretation of SSMs relating to Markov chains.; \begin{itemize} \item Markov chain with linear dynamics in latents, \item Markov chain in latents perturbed by Gaussian innovations noise (may describe stochasticity, unknown control, or model mismatch.) \item Obs are a linear projection of the dynamical state, with additive iid Gaussian noise.  \end{itemize}

In a SSM, can the observations be of higher dimension than the latents? How about in factor analysis?; \begin{itemize} \item Yes for SSM, no for FA.  \end{itemize}

Write down an example of equations for a state space model with control inputs.; \begin{itemize} \item Inputs $u_t$, Outputs $x_t$.  \item State dynamics equation: $z_t=Az_{t-1}+Bu_{t-1}+w_t$ \item Output equation: $x_t=Cz_t+Du_t+v_t$ \end{itemize}

Describe the setup of Hidden Markov Models. (including equations); \begin{itemize} \item Discrete hidden state $s_t\in \{1,...,K\}$ \item Joint $p(s_{1:T}, \mathbf{x}_{1:T})=P(s_1)P(\mathbf{x}_1|s_1)\prod_{t=2}^TP(s_t|s_{t-1})P(\mathbf{x_t}|s_t)$ \item Initial state probs $\pi_j = P(s_1=j)$ \item Transition matrix $\Phi_{ij}=P(s_{t+1}=j|s_t=i)$ \item Emission (output) dists $A_i(\mathbf{x})=P(\mathbf{x}_t=\mathbf{x}|s_t=j)$ (continuous $\mathbf{x}_t$) \item or $A_{jk}=P(\mathbf{x}_t=k|s_t=j)$ for discrete $\mathbf{x}_t$.  \end{itemize}

Write down the joint for input-outut HMMs.; \begin{itemize} \item $p(s_{1:T}, \mathbf{x}_{1:T}|u_{1:T})=P(s_1|u_1)P(\mathbf{x}_1|s_1, u_1)\prod_{t=2}^TP(s_t|s_{t-1}, u_{t-1})P(\mathbf{x_t}|s_t, u_t)$ \end{itemize}

Briefly discuss the practicality and modelling capacity of input-output HMMs.; \begin{itemize} \item Can capture arbitrarily complex input-output relationships \item but number of states required is often impractical.  \end{itemize} 

Briefly discuss input-output HMMs.; \begin{itemize} \item Can capture arbitrarily complex input-output relationships \item But number of states required is often impractical.  \end{itemize}

State one advantage and one disadvantage of LGSSMs compared to HMMs.; \begin{itemize} \item Adv: Continuous vector state is a powerful representation: a real-valued state vector can store an arbitrary number of bits in principle (vs $2^N$ states to comm N bits of information for an HMM (since states discrete, i.e. bin).) \item Disadv: LG output/dynamics are very weak: can capture v limted dynamics. HMMs can in principle represent arbitrary stochastic dynamics and output mappings.  \end{itemize}

Name at least one HMM with richer state representation than a plain HMM.; \begin{itemize} \item Factorial HMMs \item Dynamic Bayesian networks. (TODO: what is this?) \end{itemize}

Do observations $x_t$ in a HMM need to be Markov?; No.

In a HMM, is $x_1$ independent of $x_3$ conditioned on $x_2$?; Not necessarily.

Why use the Markov assumption in the latents in HMMs?; \begin{itemize} \item Cond indep makes computation much easier.  \item Because data may be partially observed: e.g. classical physics local in time(?), working out what's really out there in the world \end{itemize}

Name two interpretations of HMMs; \begin{itemize} \item Markov chain with stochastic measurements (focus on latents $s_{1:T}$ \item Mixture model with states coupled over time ($s_t, x_t$ together) \end{itemize}

For an HMM, is the output process Markov of some order?; Not necessarily.

Can we use discrete state, discrete output models (HMMs) to approximate nonlinear continuous dynamics and observation mappings?; Yes, but this is usually not practical. 

TODO: what are stochastic finite state machines / automata?; (mentioned in a one-liner)

Briefly discuss the  information one (continuous) state can hold.; Depends on the noise magnitude. 

Compare the modelling capacity of HMM dynamics and linear Gaussian dynamics with respect to fixed points.; \begin{itemize} \item HMM can converge to >1 fixed points e.g. with cycles.  \item LGSSM can only converge to one or a line of fixed points.  \end{itemize}

Name one extension of HMMs; \begin{itemize} \item (*) Switching state space models \item Hierarchical models \item Constrained HMMs \item Continuous state models with discrete outputs for time series and static data \item (as long as you remember the first one it's fine) \end{itemize}

Describe the setup of Linear Gaussian SSMs. (including equations); \begin{itemize} \item Gaussian hidden state $z_t \sim N(\mathbf{\mu_0}, Q_0)$ 
\item Transition dynamics $\mathbf{z_t|z_{t-1}}\sim  N(A\mathbf{z_{t-1}}, Q)$ \item Emission (output) dists $\mathbf{x_t|z_t}\sim N(C\mathbf{z}_t, R)$.  
\item Joint $P(\mathbf{x_1,...,x_T,z_1,...,z_T})=P(\mathbf{z_1})\prod_{t=2}^T P(\mathbf{z_t|z_{t-1}})\prod_{t=1}^T P(\mathbf{x_t|z_t})$
\end{itemize}

What information do we need when doing maximum likelihood learning with EM for LGSSMs?; \begin{itemize} \item Expectations needed in E-step are derived from singleton and pairwise marginals \item (Likewise only need singleton and pairwise expectations on q for M-step? TODO check) \item Since we can break down the log joint into pieces (since the joint has a factored structure.) \end{itemize}

Discuss how factored structure helps reduce computational cost in LGSSM models.; \begin{itemize} \item Don't need to invert matrix $(TK^3)$ cost (TODO: what matrix? Cov?) \item Only need subpieces \item And matrix has structure, can invert without explicit inversion. \item (from notes, slide 16 ML learning with EM) \end{itemize}

Name three general inference problems (and one more problem) in chain models; \begin{itemize} \item Filtering: $P(\mathbf{z_t|x_1,...,x_t})$ \item Smoothing: $P(\mathbf{z_t|x_1,...,x_T})$  \item (Also $P(\mathbf{z_t, z_{t-1}|x_1,...,x_T})$ for learning) \item Prediction: $P(\mathbf{z_t|x_1,...,x_{t-\Delta t}})$ \end{itemize}

Describe filtering; \begin{itemize} \item Identify distribution on latents using data up till current time \item  $P(\mathbf{z_t|x_1,...,x_t})$ \end{itemize}

Describe smoothing; \begin{itemize} \item Identify distribution of latent using all data. \item (Future information is informative about the past, e.g. given an airplane can't `jump', future radar data informative of where it is now.) \item $P(\mathbf{z_t|x_1,...,x_T})$ \end{itemize}

What integral may we have to calculate to evaluate $P(\mathbf{z_t|x_1,...,x_t})$? What makes us able to evaluate these in latent chain models?; \begin{itemize} \item $P(\mathbf{z_t|x_1,...,x_t}) = \int ...\int d\mathbf{z_1}...d\mathbf{z_{t-1}}P(\mathbf{y_1},...,\mathbf{y_t}|\mathbf{x_1},...,\mathbf{x_t}$ \item But the factored structure of the distributions will help us. \item The algorithms rely on a form of temporal updating or message passing. \end{itemize}

Describe a naive vs the dynamic programming way of finding $P(s_k|\mathbf{x_1,...,x_t}$. \begin{itemize} \item Naive: start one bug at each state holding, copy bug to each subsequent state, multiply by transition x output emission prob, sum all bugs per state at end timestep. (Rough description will do) \item FYI expression is $P(s_t=k|\mathbf{x_1,...,x_t})=\sum_{k_1, ..., k_{t-1}}P(s_1=k_1,...,s_t=k|\mathbf{x_1,...,x_t}\propto\sum_{k_1,...,k_{t-1}}\pi_{k_1}A_{k_1}\mathbf{x_1}\phi_{k_1,k_2}A_{k_2}(\mathbf{x_2})...\phi_{k_{t-1},k}A_k(\mathbf{x_t})$ \item Recursive (DP): at every timestep, replace bugs at each node with a single bug carrying the sum of values. \end{itemize}

Write down/derive the recursive formula for calculating $P(\mathbf{z_t}|x_{1:t})$; \begin{itemize} \item $P(\mathbf{z_t}|x_{1:t})$ \item $=\int P(\mathbf{z_t, z_{t-1}|x_t, x_{1:t-1}})d\mathbf{z_{t-1}}$ \item $=\int \frac{P(\mathbf{z_t, z_{t-1}|x_t, x_{1:t-1}})}{P(\mathbf{x_t|x_{1:t-1}}}d\mathbf{z_{t-1}}$ (Bayes' Rule) \item $\propto\int P(\mathbf{x_t| z_t, z_{t-1}, x_{1:t-1}})P(\mathbf{z_t|z_{t-1}, x_{1:t-1}}) P(\mathbf{z_{t-1}|x_{1:t-1}}) d\mathbf{z_{t-1}}$ \begin{itemize} \item Factor out $z_t, z_{t-1}$ \item consider only numerator \end{itemize} \item $=\int P(\mathbf{x_t| z_t})P(\mathbf{z_t|z_{t-1}}) P(\mathbf{z_{t-1}|x_{1:t-1}}) d\mathbf{z_{t-1}}$ \begin{itemize} \item First term: $x_t$ conditionally independent of $z_{t-1}, x_{1:t-1}$ given $z_t$ \item second term: $z_t$ cond indep of $x_{1:t-1}$ given $y_{t-1}$. \end{itemize} \item This is a forward recursion based on Bayes' rule.  \item (Complexity of update depends on dim of state and dists involved.) \end{itemize}

Write down the formulae for the HMM forward pass and state the time complexity.; \begin{itemize} \item Define $\alpha_t(i)=P(\mathbf{x_1,...,x_t}, s_t=i|\theta)$ \begin{itemize} \item Note: this is a joint vs a posterior. Can obtain posterior by normalising (divide by $\sum_k \alpha_t(k)$.) \end{itemize} \item then $\alpha_1(i)=P(s_1=i)P(x_1|s_1=i)=\pi_iA_i(\mathbf{x_1})$, \item $\alpha_{t+1}(i)=\big(\sum_{j=1}^K\alpha_t(j)\Phi_{ji}\big)A_i(\mathbf{x}_{t+1})$. \begin{itemize} \item $\Phi_{ji}=P(s_{t+1}=i|s_t=j)$ \item i.e. term in brackets is $ P(s_{t+1}=i, \mathbf{x_1,...,x_t}|\theta)$. \item $A_i(\mathbf{x}_{t+1})=P(x_{t+1}|s_{t+1}=i)$. \end{itemize} \item Compute likelihood: $P(\mathbf{x_1,...,x_T}|\theta)=\sum_{s_1,...,s_T}P(\mathbf{x_1,...,x_T}, s_1,...,s_T,\theta)=\sum_{k=1}^K\alpha_T(k)$. \begin{itemize} \item $=\sum_{k=1}^KP(s_T=k, \mathbf{x_1,...,x_T}|\theta)$ \end{itemize} \item Time complexity: $O(TK^2)$ vs naive sum $O(K^T)$ \end{itemize}

Briefly describe Kalman filtering (TODO check when finished going through L4); \begin{itemize} \item (1) Propose new belief about state $P(\mathbf{y_t|x_{1:t-1}})$. \item (2) Incorporate new datapoint $P(\mathbf{y_t|x_{1:t}})$. \item Preds look at prev prediction error, error affects est of state mean with some scaling (Kalman gain). \item Model: \begin{itemize} \item MSE est of state  \begin{itemize} \item Corr between Gaussian p(0) dist and MSE bc likelihood of Gaussian is squared error \end{itemize} \item where state process is linear,  \begin{itemize} \item (Correct posterior beliefs for Fourier?) \end{itemize} \item without specifying the noise \end{itemize}  \end{itemize}

State the equation for $P(\mathbf(y_1|x_1)$ for Kalman Filtering (LGSSM); \begin{itemize} \item Model: \begin{itemize} \item $\mathbf{y_1}\sim N(\mathbf{\mu_0},Q_0)$ \item $\mathbf{y_t|y_{t-1}}\sim N(A\mathbf{y}_{t-1}, Q)$ \item $\mathbf{x_t|y_t}\sim N(C\mathbf{y_t},R)$ \end{itemize} \item Let $\hat{\mathbf{y}}^0_1=\mathbf{\mu_0}$ and $\hat{V}^0_1=Q_0$. Then \begin{itemize} \item superscript: given data, subscript: timestep you're estimating \item Note V is posterior variance, not est var \end{itemize} \item $P(\mathbf{y_1|x_1})=N(\mathbf{\hat{y}_1^0+K_1(\mathbf{x_1}-C\hat{\mathbf{y}}^0_1}), \hat{V^0_1}-K_1C\hat{V}^0_1)=N(\mathbf{\hat{y}^1_1}, \hat{V}^1_1)$. \begin{itemize} \item Mean: mean in y space + (K: beta of y on x) $\times$ (deviation from mean in x). \item Var: TODO \item C maps y to x space. \item Kalman gain $K_1=\hat{V}^0_1C^T(C\hat{V}^0_1C^T+R)^{-1}$ \end{itemize} \end{itemize}
    
State the general update equations for Kalman Filtering (LGSSM); \begin{itemize} \item In general, we define $\hat{\mathbf{y}}^\tau_t \equiv E[\mathbf{y_t|x_1,...,x_\tau}]$ and $\hat{V}^\tau_t \equiv V[\mathbf{y_t}|x_1,...,x_\tau]$. Then \item (1) Propose new belief about state $P(\mathbf{y_t|x_{1:t-1}})=\int d\mathbf{y}_{t-1}P(\mathbf{y_t|y_{t-1}})P(\mathbf{y_{t-1}|x_{1:t-1}})=N(A\hat{y}^{t-1}_{t-1}, A\hat{V}^{t-1}_{t-1}A^T+Q)=N(\hat{\mathbf{y}}^{t-1}_t, \hat{V}^{t-1}_t)$. \item (2) Incorporate new datapoint $P(\mathbf{y_t|x_{1:t}})=N(\hat{\mathbf{y}}^{t-1}_t+K_t(\mathbf{x_t}-C\mathbf{\hat{y}}^{t-1}_t), \hat{V}^{t-1}_t - K_tC\hat{V}^{t-1}_t)=N(\mathbf{\hat{y}}^t_t, \hat{V}^t_t)$. \begin{itemize} \item Kalman gain $K_t = \hat{V}^{t-1}_tC^T(C\hat{V}^{t-1}_tC^T+R)^{-1} = \langle \mathbf{yx}^T \rangle \langle \mathbf{xx^T} \rangle^{-1}$ \item Preds look at prev prediction error, error affects est of state mean with some scaling (Kalman gain) \end{itemize} \begin{itemize} \item superscript: given data, subscript: timestep you're estimating \item Note V is posterior var, not est var \end{itemize} \item Model: \begin{itemize} \item $\mathbf{y_1}\sim N(\mathbf{\mu_0},Q_0)$ \item $\mathbf{y_t|y_{t-1}}\sim N(A\mathbf{y}_{t-1}, Q)$ \item $\mathbf{x_t|y_t}\sim N(C\mathbf{y_t},R)$ \end{itemize} \end{itemize}

What is innovations noise in a LGSSM?; \begin{itemize} \item Noise in latent state transition \item i.e. Q in $\mathbf{y_t|y_{t-1}}\sim N(A\mathbf{y_{t-1}, Q})$. \end{itemize}

Give two motivations for `Bayesian smoothing', i.e. finding the marginal posterior $P(\mathbf{y_t|x_{1:T}})$.; \begin{itemize} \item Finding the entire trajectory of a plane given radar data \item Learning \end{itemize}

Write down the general formula to find the marginal posterior $P(\mathbf{y_t|x_{1:T}})$ via Bayesian smoothing; \begin{itemize} \item $\frac{P(\mathbf{y_t, x_{t+1:T}|x_{1:t}})}{P(\mathbf{x_{t+1:T}|x_{1:t}})}$ \item $=\frac{P(\mathbf{x_{t+1:T}|y_t})P(\mathbf{y_t|x_{1:t}})}{P(\mathbf{x_{t+1:T}|x_{1:t}})}$ \item num = (bwkd x fwd). Split out $y_t$. \end{itemize}

Describe the HMM forward-backward algorithm for smoothing; \begin{itemize} \item State estimation: $\gamma_t(i)\equiv P(s_t=i|\mathbf{x_{1:T}})$ \item $=\frac{P(s_t=i, \mathbf{x_{1:t}})P(\mathbf{x_{t+1:T}|s_t=i}}{P(\mathbf{x_{1:T}}}$ \begin{itemize} \item Trick: breaking up x by time \item num = fwd x bkwd \end{itemize} \item $=\frac{alpha_t(i)\beta_t(i)}{\sum_j\alpha_t(j)\beta_t(j)}$ \item forward and backward recursions for $\alpha, \beta$ \item $\alpha_t(i)$ gives total inflow of probabilities to node (t, i), $\beta_t(i)$ gives total outflow of probabilities \end{itemize}

State / derive the backward recursion formula for $\beta_t(i)$ in the HMM forward-backward algorithm.; \begin{itemize} \item $\beta_t(i)=P(\mathbf{x_{t+1:T}|s_t=i})$ \item $=\sum_{j=1}^K P(s_{t+1}=j, \mathbf{x_{t+1},  x_{t+2:T}}|s_t=i)$ \item = $\sum_{j=1}^K P(s_{t+1}=j, s_t=i) P(\mathbf{x_{t+1}}|s_{t+1}=j)P(\mathbf{x_{t+2:T}}|s_{t+1}=j)$ \item $=\sum_{j=1}^K\Phi_{ij}A_j(\mathbf{x_{t+1}})\beta_{t+1}(j)$. \end{itemize}

Does the forward-backward algorithm give the path with the highest probability of generating the data?; \begin{itemize} \item No.  \item It gives the path with the maximum expected number of correct states. \item i.e. grade timepoint by timepoint which state has highest probability vs a single continuous path. \item Viterbi finds most probable state sequence. $\arg\max_{s_{1:T}}P(s_{1:T}|\mathbf{x_{1:T},\theta})$ \end{itemize}

Describe Viterbi decoding; \begin{itemize} \item max-product $\arg\max_{s_{1:T}}P(s_{1:T}|\mathbf{x_{1:T},\theta})$  \item (vs fwd-bkwd sum-product) \item Computes most probably state sequence \item (bug-wise: at each step, kill all bugs except for the one with the highest value at the node. \end{itemize}

TODO: Describe modified EM training based ont he Viterbi decoder and what it's good for; \begin{itemize} \item Act as though most likely path was an observed path (in practice, not in principle) \item Good bc for some models it's easier to find the max than to find the full posterior marginals \end{itemize}

Compare the forward-backward algorithm and Viterbi decoding in one sentence.; \begin{itemize} \item forward-backward: Sum-product $\arg\max_{i}P(s_t=i|\mathbf{x}_{1:T})$ vs \item Viterbi: max-product $\arg\max_{s_{1:T}}P(s_{1:T}|\mathbf{x_{1:T},\theta})$ \end{itemize}

Write down the formulae for Kalman Smoothing for the LGSSM (general x and y, no need for matrices); \begin{itemize} \item $P(\mathbf{y_t|x_{1:T}})=\int P(\mathbf{y_t, y_{t+1}|x_{1:T}}d\mathbf{y_{t+1}}$ \item $=\int P(\mathbf{y_t|y_{t+1},x_{1:T}}P(\mathbf{y_{t+1}|x_{1:T}}) d\mathbf{y_{t+1}}$ \item $=\int P(\mathbf{y_t|y_{t+1},x_{1:t}}P(\mathbf{y_{t+1}|x_{1:T}}) d\mathbf{y_{t+1}}$ \begin{itemize} \item by the Markov property. \end{itemize} \end{itemize}

Compare (in words) the smoothing algorithms for HMMs (fwd-bkwd) vs the LGSSM; \begin{itemize} \item HMM: calculate forward, backward and multiply together at the end \item LGSSM: don't bring future xs to other side (TODO?) \begin{itemize} \item  $P(\mathbf{y_t|x_{1:T}})=\int P(\mathbf{y_t|y_{t+1},x_{1:t}}P(\mathbf{y_{t+1}|x_{1:T}}) d\mathbf{y_{t+1}}$ \end{itemize} \end{itemize}

State the matrices for backward recursion for Kalman smoothing; TODO how use? guessing $N(\mathbf{\hat{y}^T_t},\hat{V}^T_t)$?  \begin{itemize} \item $J_t=\hat{V}^t_tA^T(\hat{V}^t_{t+1})^{-1}$ \begin{itemize} \item Like regression coefficient \item A maps $y_t$ to $y_{t+1}$ \end{itemize} \item $\mathbf{\hat{y}^T_t}=\hat{\mathbf{y}}^t_t+J_t(\hat{\mathbf{y}}^T_{t+1}-A\hat{\mathbf{y}}^t_t)$ \begin{itemize} \item Mean + regr coeff * (deviation?) \end{itemize} \item $\hat{V}^T_t=\hat{V}^t_t+J_t(\hat{V}^T_{t+1}-\hat{V}^t_{t+1})J_t^T$ \end{itemize}

Describe how you'd do maximum likelihood learning for SSMs (LGSSM) using batch EM.; \begin{itemize} \item E-step (max F wrt q with $\theta$ fixed): two-state extension of Kalman smoother $q^*(\mathbf{y})=p(\mathbf{y|x}, \theta)$ \item M-step: solving a few weighted least squares problems, since all the variables in $p(\mathbf{y, x}|\theta)=p(\mathbf{y_1})p(\mathbf{x_1|y_1})\prod_{t=2}^Tp(\mathbf{y_t|y_{t-1}})p(\mathbf{x_t|y_t})$ form a multivariate Gaussian. \item FYI difficulty: E-step usually harder, M-step usually easier (usually standard exponential family updates) \end{itemize}

Derive the M-step for C (maps $y_t$ to $x_t$) in the LGSSM.; (TODO expand question? not clear?) \begin{itemize} \item TODO  \end{itemize}

Derive the M-step for A (maps $y_t$ to $y_{t+1}$) in the LGSSM; TODO expand q not clear wt M-step is for?
% Bayesian Smoothing slide 27

How do the M-steps for C (maps $y_t$ to $x_t$) and A (maps $y_t$ to $y_{t+1}$) in the LGSSM model compare to factor analysis?; \begin{itemize} \item C: same as (WHAT) in factor analysis \item A: analogous to factor analysis, with expected correlations (FA has what instead?) TODO \end{itemize}

Describe how you might update the parameters of an LGSSM model online as observations arrive.; \begin{itemize} \item Differentiate conditional log likelihoods at each timestep $l_t=\ln p(\mathbf{x_t|x_1,...x_{t-1}})$ to obtain gradient rules for parameters $A, C, Q, R$ \item Note $l=\sum_{t=1}^T\ln  p(\mathbf{x_t|x_1,...x_{t-1}})=\sum_{t=1}^Tl_t$. \item An approximate version of the filtering version of q (in notes, haven't verified). Unclear what this mismatch means. \item Interpretation (from notes): instead of just max likelihood, maximising additive terms (one at a time?), may be advantageous when dynamics are nonstationary (why?) \item Size of the gradient step (lr) reflects our expectation about nonstationarity (more nonstationary, smaller step?) \end{itemize}

Describe how you'd learn HMMs using EM and state what it's called.; \begin{itemize}
\item Baum-Welch
    \item E-step: $q^*(s_{1:T})=P(s_{1:T}|\mathbf{x_{1;T}}, \theta)$
    \begin{itemize}
        \item Only need marginal probabilities $q(s_t, s_{t+1})$, which can also be obtained from the forward-backward algorithm.
    \end{itemize}
    \item M-step: Re-estimate the parameters by computing the expected number of times the HMM was in state i, emitted symbol k and transitioned to state j. (TODO?)
\end{itemize}

State the M-step updates for HMM.; \begin{itemize}
    \item $\hat{\pi}_i = \gamma_1(i) = P(s_t=i|x_{1:T})$
    \item Expected number of transitions from state i to j which begin at time t
    \begin{itemize}
        \item $\xi_t(i\rightarrow j) \equiv P(s_t=i, s_{t+1}=j|\mathbf{x_{1:T}})$
        \item $=\alpha_t(i)\Phi_{ij}A_j(\mathbf{x_{t+1}})\beta_{t
        1}(j)/P(x_{1:T})$
    \end{itemize}
    \item So the estimated transition probabilities are
    \begin{itemize}
        \item $\hat{\Phi_{ij}}=\sum_{t=1}^{T-1}\xi_t(i\rightarrow j)/\sum_{t=1}^{T-1}\gamma_t(i)$
    \end{itemize}
    \item Output dists are expected number of times we observe a particular symbol in a particular state:
    \begin{itemize}
        \item $\hat{A}_{ik}=\sum_{t:\mathbf{x}_t=k}\gamma_t(i)\big / \sum_{t=1}^T\gamma_t(i)$
        \item = number of times made obs k and state was i (joint) div by (prob state was i)
        \item (or the state-probability weighted mean and variance for a Gaussian output model)
    \end{itemize}
\end{itemize}

Describe a practical thing to do when doing EM for HMMs.; Numerical scaling \begin{itemize}
    \item The conventional message definition is in terms of a large joint: $\alpha_t(i)=P(\mathbf{x_{1:t}},s_t=i)\rightarrow 0$ as t grows, and so can easily underflow.
    \item Rescale: $\bar{\alpha}_t=A_i(\mathbf{x_t})\sum_j\tilde{\alpha}_{t-1}(j)\Phi_{ji}$, \begin{itemize}
        \item $\rho_t = \sum_{i=1}^K\bar{\alpha}_t(i)$
        \item $\tilde{alpha}_t(i)=\bar{\alpha}_t(i)/\rho_t$
        \item Can show that $\rho_t=P(\mathbf{x_t|x_{1:t-1}}, \theta)$
        \item and $\prod_{t=1}^T\rho_t = P(\mathbf{x_{1:T}}|\theta)$
    \end{itemize}
    \item Can also represent as log
\end{itemize}

\end{document}
