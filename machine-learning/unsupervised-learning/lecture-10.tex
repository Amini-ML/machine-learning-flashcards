%Front;
% Approximate Inference
% Lectured by Maneesh Sahani at the Gatsby Computational Neuroscience Unit, UCL, Fall 2018
% Link to course info and slides: http://www.gatsby.ucl.ac.uk/teaching/courses/ml1/

\documentclass{article}
\begin{document}

Name five ways of approximating the forward messages on a latent chain model / NLSSM.; \begin{itemize}
    \item Linearisation
    \item Laplace filter: use mode and curvature of integrand (Hessian)
    \item Sigma-point (`unscented' filter) \begin{itemize}
        \item 'Fit' Gaussian to 2K+1 points $f(\hat{\mathbf{y}}_{t-1}), f(\hat{\mathbf{y}}_{t-1}\pm \sqrt{lambda}\mathbf{v})$ for evs, evectors $\hat{V}_{t-1}\mathbf{v}=\lambda\mathbf{v}$ (mode of pred and set of 1 sigma pts along major axes of cov, which are called sigma points)
        \item (assumes msgs are Gaussian, like moment matching)
        \item bc pts equivalent to numerical evaluation of mean and cov by Gaussian quadrature
        \item (one form of assumed density filtering and EP)
    \end{itemize}
    \item Parametric variational: $\arg\min KL [N(\mathbf{\hat{y}}_t, \hat{V}_t)||\int dy_{t-1}...]$. \begin{itemize}
        \item Requires Gaussian expectations of $\log \int$, so may be challenging.
    \end{itemize}
    \item EP: other KL: $\arg\min KL[\int  dy_{t-1} ||N(\hat{\mathbf{y}}_t, \hat{V}_t)]$, needs only first and second moments of nonlinear message.
    \begin{itemize}
        \item Like moment matching in a more iterative way
    \end{itemize}
    \item Latter two approaches more principled than linearisation: look for an approx q that is closest to posterior P in some sense: $q=\arg\min_{q\in\mathcal{Q}}D(P\leftrightarrow q)$: D and $\mathcal{Q}$ open choices.
\end{itemize}

% from s7 The other KL.
Discuss in what ways minimising KL(p||q) may or may not be `correct'; TODO \begin{itemize}
    \item 'for a factored approx the clique marginals obtained by minimising this KL are correct': $\arg\min_{q_i} KL[P(\mathcal{Y}|\mathcal{X}||\prod q_j(\mathcal{Y}_j|\mathcal{X})]=P(\mathcal{Y}_i|\mathcal{X})$
    \item If do exactly, while (1) may lose correl (??) bc of factored fn,
    \item marginals are exact
\end{itemize} 

Show that for a factored approximation, the clique marginals obtained by minimising $KL(p||q)$ are correct; i.e. show $\arg\min_{q_i} KL[P(\mathcal{Y}|\mathcal{X}||\prod q_j(\mathcal{Y}_j|\mathcal{X})]=P(\mathcal{Y}_i|\mathcal{X})$. Steps: \begin{itemize}
    \item $\arg\min_{q_i}-\int d\mathcal{Y}P(\mathcal{Y}|\mathcal{X})\log \prod_iq_j(\mathcal{Y}_j|\mathcal{X})$
    \begin{itemize}
        \item Assume corr to cliques, not necc of joint?
    \end{itemize}
    \item $\arg\min_{q_i}-\sum_j \int d\mathcal{Y}P(\mathcal{Y}|\mathcal{X})\log q_j(\mathcal{Y}_j|\mathcal{X})$
    \item $\arg\min_{q_i}-\int d\mathcal{Y_i}P(\mathcal{Y_i}|\mathcal{X})\log q_i(\mathcal{Y}_i|\mathcal{X})$
    \begin{itemize}
        \item bc expectation only depends on $\mathcal{Y}_i$ terms
        \item doesn't necc assume $P(\mathcal{Y}|\mathcal{X})$ factors, just integrate over the rest?
    \end{itemize}
    \item (so finding the best q for this KL is intractable.)
\end{itemize}

How can we find $q=\arg\min KL(p||q)$?; it's intractable.

Write the posterior distribution in a graphical model as a (normalised) product of factors.;\begin{itemize}
    \item  $P(\mathcal{Y}|\mathcal{X})=\frac{P(\mathcal{Y}, \mathcal{X})}{P(\mathcal{X})}=\frac{1}{Z}\prod_i P(Y_i|pa(Y_i))\propto \prod_{i=1}^N f_i(\mathcal{Y}_i)$.
    \item where the $\mathcal{Y}_i$ are not necessarily disjoint.
\end{itemize}

What are sites in EP?; Factors.

What is the form of $q$ in EP?; $q(\mathcal{Y}):=\prod_{i=1}^N\tilde{f}_i(\mathcal{Y}_i)$, where $f_i$ are factors/sites. $\tilde{f}_i$ are approximated $f_i$.

What do we minimise in EP to get $q$? Describe the intuition behind it.; \begin{itemize}
    \item $\arg\min_{\tilde{f}_i}KL[f_i(\mathcal{Y}_i)\prod_{j\ne i}\tilde{f}_j(\mathcal{Y}_j)||\tilde{f}_i(\mathcal{Y}_i)\prod_{j\ne i}\tilde{f_j}(\mathcal{Y}_j)]$
    \item $=\arg\min_{\tilde{f}_i}KL[f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})||\tilde{f}_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})]$
    \item local (opt $\tilde{f}_i$ each time), contextual (prod with other ests $\tilde{f}_j$): iterative, accurate
    \item intuition: obs in one clique may say bump is at X, whereas all  other cliques may say bump is at Y, should factor that in.
\end{itemize}

Describe the two ideas behind EP.; \begin{itemize}
    \item E: Approximation of factors: \begin{itemize}
        \item Usually by `projection' to exponential families (proj bc info geom view of expfams)
        \item This involves finding expected sufficient statistics, hence expectation
    \end{itemize}
    \item P: Local $(KL(p||q))$ divergence minimisation in the context of other factors (This leads to a message passing appraoch, hence propagation)
\end{itemize}

Simplify $\arg\min_{\tilde{f}_i}KL[f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})||\tilde{f}_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})]$; \begin{itemize}
    \item $=\arg\max_{\tilde{f}_i} \int d\mathcal{Y}_i d\mathcal{Y}_{\neg i} f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})\log \tilde{f}_i(\mathcal{Y}_i) q_{\neg i}(\mathcal{Y})$
    \item $=\arg\max_{\tilde{f}_i} \int d\mathcal{Y}_i d\mathcal{Y}_{\neg i} f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_{\neg i}|\mathcal{Y}_i)(\log \tilde{f}_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)+\log q_{\neg i}(\mathcal{Y}_{\neg i}|\mathcal{Y}_i))$
    \item $=\arg\max_{\tilde{f}_i} \int d\mathcal{Y}_i f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)(\log \tilde{f}_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)) \int d\mathcal{Y}_{\neg i} q_{\neg i}(\mathcal{Y}_{\neg i}|\mathcal{Y}_i)$
    \item $=\arg\min_{\tilde{f}_i}KL[f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)||\tilde{f}_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)]$
\end{itemize}

What is the cavity distribution in EP?; $q_{\neg i}(\mathcal{Y}_i)$

Write the EP algorithm in pseudocode.; \begin{itemize}
    \item Input $f_1(\mathcal{Y}_1), ..., f_N(\mathcal{Y}_N)$
    \item Initialise $\tilde{f}_1(\mathcal{Y}_1)=\arg\min_{f\in\{\tilde{f}\}} KL[\bar{f}_1(\mathcal{Y}_1||f_1(\mathcal{Y}_1)], \tilde{f}_i(\mathcal{Y}_i)=1$ for $i>1, q(\mathcal{Y})\propto\prod_i\tilde{f}_i(\mathcal{Y}_i)$.
    \begin{itemize}
        \item first term $\bar{f}_1$ is fixed, true one. Latter $f$ is one to opt over.
        \item init to be uniform
    \end{itemize}
    \item repeat
    \begin{itemize}
        \item for $i=1,...,N$ do
        \begin{itemize}
            \item Delete $q_{\neg i}(\mathcal{Y})\leftarrow \frac{q(\mathcal{Y}}{\tilde{f}_i(\mathcal{Y}_i)}=\prod_{j\ne i}\tilde{f}_j(\mathcal{Y}_j)$
            \item Project $\tilde{f}_i^{new}(\mathcal{Y})\leftarrow \arg\min_{f\in \{\tilde{f}\}} KL[f_i(\mathcal{Y}_iq_{\neg i}(\mathcal{Y}_i)||f(\mathcal{Y}_iq_{\neg i}(\mathcal{Y}_i)]$
            \item Include: $q(\mathcal{Y}_i)\leftarrow \tilde{f}_i^{new}(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})$
        \end{itemize}
        \item end for
    \end{itemize}
    \item until convergence
\end{itemize}

How can we calculate $q_{\neg i}(\mathcal{Y}_i)$ in EP more efficiently?; \begin{itemize}
    \item break down into $\prod_{j\in ne(i)}M_{j\rightarrow i}(\mathcal{Y}_j \cap \mathcal{Y}_i)$
    \item i.e. prod of terms from each neighbouring clique
    \item (once the $i$th site has been approximated, the messages can be passed on to neighbouring cliques by marginalising the shared variables.
    \item so don't have to marginalise over each combo of vars
    \item TODO: not sure if there is a better ans to this q
\end{itemize}

Break down the cavity distribution $q_{\neg i}(\mathcal{Y}_i)$ in a tree; \begin{itemize}
    \item $q_{\neg i}(\mathcal{Y}_i) = \prod_{j\in ne(i)}M_{j\rightarrow i}(\mathcal{Y}_j \cap \mathcal{Y}_i)$
    \item i.e. prod of terms from each neighbouring clique
    \item (once the $i$th site has been approximated, the messages can be passed on to neighbouring cliques by marginalising the shared variables.
\end{itemize}

Is loopy BP (using cavity distributions as in EP) guaranteed to converge?; No.

In loopy graphs, what does $q_{\neg i}(\mathcal{Y}_i)$ become?; \begin{itemize}
    \item Recall $q_{\neg i}(\mathcal{Y}_i) = \prod_{j\in ne(i)}M_{j\rightarrow i}(\mathcal{Y}_j \cap \mathcal{Y}_i)$
    \item becomes an approx to the true cavity distribution (or we can recast the approx directly in terms of messages, see a later lecture
    \item (For some approx e.g. Gaussian, may be able to compute true loopy cavity using approx sites, even if computing exact messages would have been intractable.)
\end{itemize}

% TODO s13: EP for a NLSSM equations, s14 more on that

Describe EP for a NLSSM; \begin{itemize}
    \item Put $y_t, y_{t-1}, x_t$ in the same factor.
    \item Let $P(\mathbf{y_i|y_{i-1}}) = \phi_i(\mathbf{y_i, y_{i-1}}), P(\mathbf{x_i|y_i}) = \phi_i(\mathbf{y_i})$. Then $f_i(\mathbf{y_i, y_{i-1}}) = \phi_i(\mathbf{y_i, y_{i-1}})\phi_i(\mathbf{y_i})$. As $\phi_i$ and $\psi_i$ are non-linear, inference is not generally tractable.
    \item Assume $\tilde{f}_i(\mathbf{y_i, y_{i-1}})$ is Gaussian. Then find form of $q_{\neg i}(\mathbf{y_i, y_{i-1}}) = \alpha_{i-1}(\mathbf{y_{i-1}})\beta_i(\mathbf{y}_i)$, with both $\alpha, \beta$ Gaussian. \begin{itemize}
        \item steps: $=\int_{\mathbf{y_{1:i-2, i+1:n}}} \prod_{i'\ne i}\tilde{f}_{i'}(\mathbf{y_{i'}, y_{i'-1}})$
        \item $=\int_{\mathbf{y_{1:i-2}}} \prod_{i< i}\tilde{f}_{i'}(\mathbf{y_{i'}, y_{i'-1}}) \int_{\mathbf{y_{i+1:n}}} \prod_{i'> i}\tilde{f}_{i'}(\mathbf{y_{i'}, y_{i'-1}})$
    \end{itemize}
    \item $\tilde{f}_i(\mathbf{y_i, y_{i-1}}) = \arg\min_{f\in N} KL[\phi_i(\mathbf{y_i, y_{i-1}})\psi_i(\mathbf{y_i})\alpha_{i-1}(\mathbf{y_{i-1}})\beta_i(\mathbf{y_i}) || f(\mathbf{y_i, y_{i-1}})\alpha_{i-1}(\mathbf{y_{i-1}})\beta_i(\mathbf{y}_i)]$
    \item update $\alpha, \beta, \tilde{P}, \tilde{f}$. \begin{itemize}
        \item $\tilde{f}_i(\mathbf{y_i, y_{i-1}}) = \frac{\tilde{P}(\mathbf{y_{i-1}, y_i})}{\alpha_{i-1}(\mathbf{y_{i-1}})\beta_i(\mathbf{y_i})}$
    \end{itemize} 
\end{itemize}

Derive the EP message update for $\alpha$; \begin{itemize}
    \item $\alpha_i(\mathbf{y_i}) = \int_{\mathbf{y_{1:i-1}}} \int \prod_{i'<i+1} \tilde{f}_{i'}(\mathbf{y_{i'}, \mathbf{y}_{i'-1}}) 
    $
    \item $= \int_{\mathbf{y_{i-1}}}\alpha_{i-1}(\mathbf{y_{i-1}})\tilde{f}_i(\mathbf{y_i, y_{i-1}})$
    \item $= \frac{1}{\beta_i(\mathbf{y}_i)}\int_{y_{i-1}}\tilde{P}(\mathbf{y_{i-1},y_i})$.
    \item (Recall $\tilde{f}_i(\mathbf{y_i, y_{i-1}}) = \frac{\tilde{P}(\mathbf{y_{i-1}, y_i})}{\alpha_{i-1}(\mathbf{y_{i-1}})\beta_i(\mathbf{y_i})}$)
\end{itemize} 

Derive the EP message update for $\beta$; \begin{itemize}
    \item $\beta_{i-1}(\mathbf{y_{i-1}}) = \int_{\mathbf{y_{i:n}}} \int \prod_{i'>i} \tilde{f}_{i'}(\mathbf{y_{i'}, \mathbf{y}_{i'-1}}) 
    $
    \item $= \int_{\mathbf{y_{i}}}\beta_{i}(\mathbf{y_{i}})\tilde{f}_i(\mathbf{y_i, y_{i-1}})$
    \item $= \frac{1}{\alpha_{i-1}(\mathbf{y}_{i-1})}\int_{y_{i}}\tilde{P}(\mathbf{y_{i-1},y_i})$.
    \item (Recall $\tilde{f}_i(\mathbf{y_i, y_{i-1}}) = \frac{\tilde{P}(\mathbf{y_{i-1}, y_i})}{\alpha_{i-1}(\mathbf{y_{i-1}})\beta_i(\mathbf{y_i})}$)
\end{itemize} 

Derive the EP message updates for $\tilde{f}, \tilde{P}$. Assume you know $\alpha, \beta$.; \begin{itemize}
    \item $\tilde{f}_i(\mathbf{y_i, y_{i-1}}) = \frac{\tilde{P}(\mathbf{y_{i-1}, y_i})}{\alpha_{i-1}(\mathbf{y_{i-1}})\beta_i(\mathbf{y_i})}$
\end{itemize}

How do you find q to minimise $KL(p(x)||q(x))$ when q is an exponential family  distribution?; \begin{itemize}
    \item Let $q(x) = \frac{1}{Z(\theta)}e^{T(x)\cdot\theta}$. Then
    \item $\arg\min_q KL[p(x)||q(x)] = \arg\min_{\theta} KL[p(x)||\frac{1}{Z(\theta)e^{T(x)\dcdot\theta}}]$
    \item $= \arg\min_{\theta} - \int dx p(x) \log \frac{1}{Z(\theta)}e^{T(x)\cdot\theta}$
    \item $= \arg\min_{\theta} - \int dx p(x)T(x)\cdot \theta + \log Z(\theta)$
    \item $\frac{\partial}{\partial \theta} = \int dx p(x) T(x) + \frac{1}{Z(\theta)}\int dx e^{T(x)\cdot \theta} T(x)$
    \item $= -\langle T(x) \rangle_p + \frac{1}{Z(\theta)}\int dx e^{T(x)\cdot \theta} T(x)$
    \item $ = - \langle T(x)\rangle_p + \langle T(x) \rangle_q$.
    \item so the minimum is found by matching sufficient stats. This is usually moment matching.
\end{itemize}


How do we calculate $\langle T(x) \rangle_p$ for EP?; \begin{itemize}
    \item Often analytically tractable, 
    \item but even if not, requires a relatively low dim integral 
    \item (Numerical issues: mostly lack of posdef-ness)
    \item can use \begin{itemize}
        \item (1) Quadrature methods or 
        \begin{itemize}
            \item Classical Gaussian (not to do with dist) quadrature gives an iterative version of sigma-point methods
            \item posdef joints, but not guaranteed posdef messages -> can skip non-posdef steps or damp msgs by interpolation or exponentiating to power < 1. (TODO how does exp/interpolating help?)
            \item other quadrature methods like GP quadrature may be more accurate and  may allow formal constraint to posdef cone.
        \end{itemize}
        \item (2) Laplace approx: as long as msgs remain posdef, will converge to global Laplace approx
    \end{itemize}
\end{itemize}

What does a GP provide a prior over?; Functions.

% TODO: s16-18 EP for GP classification
Describe how to use EP for Gaussian Process classification.; \begin{itemize}
    \item 
\end{itemize}

% TODO s17 normalisers

% TODO s18 Alpha divergences and power EP



\end{document}