%Front;
% Approximate Inference
% Lectured by Maneesh Sahani at the Gatsby Computational Neuroscience Unit, UCL, Fall 2018
% Link to course info and slides: http://www.gatsby.ucl.ac.uk/teaching/courses/ml1/

\documentclass{article}
\begin{document}

Name five ways of approximating the forward messages on a latent chain model / NLSSM.; \begin{itemize}
    \item Linearisation
    \item Laplace filter: use mode and curvature of integrand (Hessian)
    \item Sigma-point (`unscented' filter) \begin{itemize}
        \item 'Fit' Gaussian to 2K+1 points $f(\hat{\mathbf{y}}_{t-1}), f(\hat{\mathbf{y}}_{t-1}\pm \sqrt{lambda}\mathbf{v})$ for evs, evectors $\hat{V}_{t-1}\mathbf{v}=\lambda\mathbf{v}$ (mode of pred and set of 1 sigma pts along major axes of cov, which are called sigma points)
        \item (assumes msgs are Gaussian, like moment matching)
        \item bc pts equivalent to numerical evaluation of mean and cov by Gaussian quadrature
        \item (one form of assumed density filtering and EP)
    \end{itemize}
    \item Parametric variational: $\arg\min KL [N(\mathbf{\hat{y}}_t, \hat{V}_t)||\int dy_{t-1}...]$. \begin{itemize}
        \item Requires Gaussian expectations of $\log \int$, so may be challenging.
    \end{itemize}
    \item EP: other KL: $\arg\min KL[\int  dy_{t-1} ||N(\hat{\mathbf{y}}_t, \hat{V}_t)]$, needs only first and second moments of nonlinear message.
    \begin{itemize}
        \item Like moment matching in a more iterative way
    \end{itemize}
    \item Latter two approaches more principled than linearisation: look for an approx q that is closest to posterior P in some sense: $q=\arg\min_{q\in\mathcal{Q}}D(P\leftrightarrow q)$: D and $\mathcal{Q}$ open choices.
\end{itemize}

% from s7 The other KL.
Discuss in what ways minimising KL(p||q) may or may not be `correct'; TODO \begin{itemize}
    \item 'for a factored approx the clique marginals obtained by minimising this KL are correct': $\arg\min_{q_i} KL[P(\mathcal{Y}|\mathcal{X}||\prod q_j(\mathcal{Y}_j|\mathcal{X})]=P(\mathcal{Y}_i|\mathcal{X})$
    \item If do exactly, while (1) may lose correl (??) bc of factored fn,
    \item marginals are exact
\end{itemize} 

Show that for a factored approximation, the clique marginals obtained by minimising $KL(p||q)$ are correct; i.e. show $\arg\min_{q_i} KL[P(\mathcal{Y}|\mathcal{X}||\prod q_j(\mathcal{Y}_j|\mathcal{X})]=P(\mathcal{Y}_i|\mathcal{X})$. Steps: \begin{itemize}
    \item $\arg\min_{q_i}-\int d\mathcal{Y}P(\mathcal{Y}|\mathcal{X})\log \prod_iq_j(\mathcal{Y}_j|\mathcal{X})$
    \begin{itemize}
        \item Assume corr to cliques, not necc of joint?
    \end{itemize}
    \item $\arg\min_{q_i}-\sum_j \int d\mathcal{Y}P(\mathcal{Y}|\mathcal{X})\log q_j(\mathcal{Y}_j|\mathcal{X})$
    \item $\arg\min_{q_i}-\int d\mathcal{Y_i}P(\mathcal{Y_i}|\mathcal{X})\log q_i(\mathcal{Y}_i|\mathcal{X})$
    \begin{itemize}
        \item bc expectation only depends on $\mathcal{Y}_i$ terms
        \item doesn't necc assume $P(\mathcal{Y}|\mathcal{X})$ factors, just integrate over the rest?
    \end{itemize}
    \item (so finding the best q for this KL is intractable.)
\end{itemize}

How can we find $q=\arg\min KL(p||q)$?; it's intractable.

Write the posterior distribution in a graphical model as a (normalised) product of factors.;\begin{itemize}
    \item  $P(\mathcal{Y}|\mathcal{X})=\frac{P(\mathcal{Y}, \mathcal{X})}{P(\mathcal{X})}=\frac{1}{Z}\prod_i P(Y_i|pa(Y_i))\propto \prod_{i=1}^N f_i(\mathcal{Y}_i)$.
    \item where the $\mathcal{Y}_i$ are not necessarily disjoint.
\end{itemize}

What are sites in EP?; Factors.

What is the form of $q$ in EP?; $q(\mathcal{Y}):=\prod_{i=1}^N\tilde{f}_i(\mathcal{Y}_i)$, where $f_i$ are factors/sites. $\tilde{f}_i$ are approximated $f_i$.

What do we minimise in EP to get $q$? Describe the intuition behind it.; \begin{itemize}
    \item $\arg\min_{\tilde{f}_i}KL[f_i(\mathcal{Y}_i)\prod_{j\ne i}\tilde{f}_j(\mathcal{Y}_j)||\tilde{f}_i(\mathcal{Y}_i)\prod_{j\ne i}\tilde{f_j}(\mathcal{Y}_j)]$
    \item $=\arg\min_{\tilde{f}_i}KL[f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})||\tilde{f}_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})]$
    \item local (opt $\tilde{f}_i$ each time), contextual (prod with other ests $\tilde{f}_j$): iterative, accurate
    \item intuition: obs in one clique may say bump is at X, whereas all  other cliques may say bump is at Y, should factor that in.
\end{itemize}

Describe the two ideas behind EP.; \begin{itemize}
    \item E: Approximation of factors: \begin{itemize}
        \item Usually by `projection' to exponential families (proj bc info geom view of expfams)
        \item This involves finding expected sufficient statistics, hence expectation
    \end{itemize}
    \item P: Local $(KL(p||q))$ divergence minimisation in the context of other factors (This leads to a message passing appraoch, hence propagation)
\end{itemize}

Simplify $\arg\min_{\tilde{f}_i}KL[f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})||\tilde{f}_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})]$; \begin{itemize}
    \item $=\arg\max_{\tilde{f}_i} \int d\mathcal{Y}_i d\mathcal{Y}_{\neg i} f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})\log \tilde{f}_i(\mathcal{Y}_i) q_{\neg i}(\mathcal{Y})$
    \item $=\arg\max_{\tilde{f}_i} \int d\mathcal{Y}_i d\mathcal{Y}_{\neg i} f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_{\neg i}|\mathcal{Y}_i)(\log \tilde{f}_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)+\log q_{\neg i}(\mathcal{Y}_{\neg i}|\mathcal{Y}_i))$
    \item $=\arg\max_{\tilde{f}_i} \int d\mathcal{Y}_i f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)(\log \tilde{f}_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)) \int d\mathcal{Y}_{\neg i} q_{\neg i}(\mathcal{Y}_{\neg i}|\mathcal{Y}_i)$
    \item $=\arg\min_{\tilde{f}_i}KL[f_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)||\tilde{f}_i(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y}_i)]$
\end{itemize}

What is the cavity distribution in EP?; $q_{\neg i}(\mathcal{Y}_i)$

Write the EP algorithm in pseudocode.; \begin{itemize}
    \item Input $f_1(\mathcal{Y}_1), ..., f_N(\mathcal{Y}_N)$
    \item Initialise $\tilde{f}_1(\mathcal{Y}_1)=\arg\min_{f\in\{\tilde{f}\}} KL[\bar{f}_1(\mathcal{Y}_1||f_1(\mathcal{Y}_1)], \tilde{f}_i(\mathcal{Y}_i)=1$ for $i>1, q(\mathcal{Y})\propto\prod_i\tilde{f}_i(\mathcal{Y}_i)$.
    \begin{itemize}
        \item first term $\bar{f}_1$ is fixed, true one. Latter $f$ is one to opt over.
        \item init to be uniform
    \end{itemize}
    \item repeat
    \begin{itemize}
        \item for $i=1,...,N$ do
        \begin{itemize}
            \item Delete $q_{\neg i}(\mathcal{Y})\leftarrow \frac{q(\mathcal{Y}}{\tilde{f}_i(\mathcal{Y}_i)}=\prod_{j\ne i}\tilde{f}_j(\mathcal{Y}_j)$
            \item Project $\tilde{f}_i^{new}(\mathcal{Y})\leftarrow \arg\min_{f\in \{\tilde{f}\}} KL[f_i(\mathcal{Y}_iq_{\neg i}(\mathcal{Y}_i)||f(\mathcal{Y}_iq_{\neg i}(\mathcal{Y}_i)]$
            \item Include: $q(\mathcal{Y}_i)\leftarrow \tilde{f}_i^{new}(\mathcal{Y}_i)q_{\neg i}(\mathcal{Y})$
        \end{itemize}
        \item end for
    \end{itemize}
    \item until convergence
\end{itemize}

How can we calculate $q_{\neg i}(\mathcal{Y}_i)$ in EP more efficiently?; \begin{itemize}
    \item break down into $\prod_{j\in ne(i)}M_{j\rightarrow i}(\mathcal{Y}_j \cap \mathcal{Y}_i)$
    \item i.e. prod of terms from each neighbouring clique
    \item (once the $i$th site has been approximated, the messages can be passed on to neighbouring cliques by marginalising the shared variables.
    \item so don't have to marginalise over each combo of vars
    \item TODO: not sure if there is a better ans to this q
\end{itemize}

Break down $q_{\neg i}(\mathcal{Y}_i)$ into messages; \begin{itemize}
    \item $\prod_{j\in ne(i)}M_{j\rightarrow i}(\mathcal{Y}_j \cap \mathcal{Y}_i)$
    \item i.e. prod of terms from each neighbouring clique
    \item (once the $i$th site has been approximated, the messages can be passed on to neighbouring cliques by marginalising the shared variables.
\end{itemize}

% TODO: loopy graphs part on slide 12, may leave till after typing LBP notes

% TODO s13: EP for a NLSSM equations, s14 more on that

% TODO s14 moment matching sort of proof

How do we calculate $\langle T(x) \rangle_p$ for EP?; \begin{itemize}
    \item Often analytically tractable, 
    \item but even if not, requires a relatively low dim integral 
    \item (Numerical issues: mostly lack of posdef-ness)
    \item can use \begin{itemize}
        \item (1) Quadrature methods or 
        \begin{itemize}
            \item Classical Gaussian (not to do with dist) quadrature gives an iterative version of sigma-point methods
            \item posdef joints, but not guaranteed posdef messages -> can skip non-posdef steps or damp msgs by interpolation or exponentiating to power < 1. (TODO how does exp/interpolating help?)
            \item other quadrature methods like GP quadrature may be more accurate and  may allow formal constraint to posdef cone.
        \end{itemize}
        \item (2) Laplace approx: as long as msgs remain posdef, will converge to global Laplace approx
    \end{itemize}
\end{itemize}

What does a GP provide a prior over?; Functions.

Describe how to use EP for Gaussian Process classification.; \begin{itemize}
    \item 
\end{itemize}



\end{document}